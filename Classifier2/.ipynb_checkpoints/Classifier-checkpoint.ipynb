{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports necessary for running the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df = pd.read_excel(\"MLDataSet.xlsx\")\\ndf2 = pd.read_excel(\"ADRs.xlsx\")\\ndf3 = pd.read_excel(\"DS.xlsx\")\\ndf4 = pd.read_excel(\"Mental.xlsx\")\\n\\ndf5=[df, df2,  df3, df4]\\n\\nresult = pd.concat([df,df2,df3, df4], axis=1, join_axes=[df.index])'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun May  5 23:01:42 2019\n",
    "\n",
    "@author: Ahmed\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "'''df = pd.read_excel(\"MLDataSet.xlsx\")\n",
    "df2 = pd.read_excel(\"ADRs.xlsx\")\n",
    "df3 = pd.read_excel(\"DS.xlsx\")\n",
    "df4 = pd.read_excel(\"Mental.xlsx\")\n",
    "\n",
    "df5=[df, df2,  df3, df4]\n",
    "\n",
    "result = pd.concat([df,df2,df3, df4], axis=1, join_axes=[df.index])'''\n",
    "#result.to_excel('result.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeColumns(result):\n",
    "    del result['Pain']\n",
    "    del result['Content']\n",
    "    del result['Filtered']\n",
    "    del result['Stemmed']\n",
    "    del result['big1']\n",
    "    del result['big2']\n",
    "    del result['small1']\n",
    "    del result['small2']\n",
    "    del result['Height']\n",
    "    del result['Joined']\n",
    "    del result['Posted']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizeCount(result):\n",
    "    '''for i in range(len(result)):\n",
    "        if result[i]==0:\n",
    "            result[i]=0\n",
    "        if result[i]>0 and result[i]<=3:\n",
    "            result[i]=1\n",
    "        if result[i]>3:\n",
    "            result[i]=2'''\n",
    "    for i in range(len(result)):\n",
    "        if result.at[i,'MentalCount']==0:\n",
    "            result.at[i,'MentalCount']=0\n",
    "        if result.at[i,'MentalCount']>0 and result.at[i,'MentalCount']<=3:\n",
    "            result.at[i,'MentalCount']=1\n",
    "        if result.at[i,'MentalCount']>3:\n",
    "            result.at[i,'MentalCount']=2\n",
    "\n",
    "\n",
    "        if result.at[i,'DieaseCount']==0:\n",
    "            result.at[i,'DieaseCount']=0\n",
    "        if result.at[i,'DieaseCount']>0 and result.at[i,'DieaseCount']<=3:\n",
    "            result.at[i,'DieaseCount']=1\n",
    "        if result.at[i,'DieaseCount']>3:\n",
    "            result.at[i,'DieaseCount']=2\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manipulating data to prepare for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manipulate(result,labelfile,label):\n",
    "    from collections import Counter\n",
    "    imp_mean = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "    result['Gender']=imp_mean.fit_transform(result['Gender'].values.reshape(-1, 1))\n",
    "    for i in range(len(result)):\n",
    "        if result.at[i,'Gender']=='Male':\n",
    "            result.at[i,'Gender']=0\n",
    "        if result.at[i,'Gender']=='Female':\n",
    "            result.at[i,'Gender']=1\n",
    "    \n",
    "    labels=labelfile.iloc[:][label]\n",
    "    print(Counter(labels))\n",
    "    labels=LabelEncoder().fit_transform(labels)\n",
    "    if 'Unnamed: 0' in result:\n",
    "        del result['Unnamed: 0'] \n",
    "    #removeColumns(result)\n",
    "    imp_mean = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "    result['Age']=imp_mean.fit_transform(result['Age'].values.reshape(-1, 1))\n",
    "    if 'Height' in result:\n",
    "        result['Height']=imp_mean.fit_transform(result['Height'].values.reshape(-1, 1))\n",
    "    result=vectorizeCount(result)\n",
    "    return result,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#asklabel=pd.read_excel('asklabels.xlsx')\n",
    "#labels=asklabel.iloc[:]['MICROCEPHALY , EPILEPSY , AND DIABETES SYNDROME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def SMOTE(begin,columns):\n",
    "    from imblearn.over_sampling import SMOTENC\n",
    "    x=[1]\n",
    "    x.extend(list(range(begin,len(columns))))\n",
    "    sm=SMOTENC(random_state=42, categorical_features=x)\n",
    "    return sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "label='ADRCount'\n",
    "asklabel='ADRCount'\n",
    "\n",
    "#from collections import Counter\n",
    "#X_res, y_res = sm.fit_resample(result, result_labels)\n",
    "#print(Counter(result_labels))\n",
    "#print(Counter(y_res))\n",
    "def prepareDataset(label,asklab):\n",
    "    from imblearn.over_sampling import SMOTENC\n",
    "    \n",
    "    \n",
    "    \n",
    "    result=pd.read_excel('result_reduced.xlsx')\n",
    "    weighted=pd.read_excel('Weighted_reduced.xlsx')\n",
    "    blood=pd.read_excel('blood_reduced.xlsx')\n",
    "    ask=pd.read_excel('askapatient_reduced.xlsx')\n",
    "\n",
    "    resultlabel=pd.read_excel('resultlabels.xlsx')\n",
    "    weightedlabel=pd.read_excel('Weightedlabels.xlsx')\n",
    "    bloodlabel=pd.read_excel('bloodlabels.xlsx')\n",
    "    asklabel=pd.read_excel('asklabels.xlsx')\n",
    "\n",
    "    result,result_labels=manipulate(result,resultlabel,label)\n",
    "    weighted,weighted_labels=manipulate(weighted,weightedlabel,label)\n",
    "    blood,blood_labels=manipulate(blood,bloodlabel,label)\n",
    "    ask,ask_labels=manipulate(ask,asklabel,asklab)\n",
    "    del result['weights2']\n",
    "    del result['Height']\n",
    "    \n",
    "    sm1 = SMOTE(2,result.columns)\n",
    "    sm2 = SMOTE(4,weighted.columns)\n",
    "    sm3 = SMOTE(4,blood.columns)\n",
    "    sm4 = SMOTE(2,ask.columns)\n",
    "    #Applying SMOTENC\n",
    "    result,result_labels=sm1.fit_resample(result,result_labels)\n",
    "    weighted,weighted_labels=sm2.fit_resample(weighted,weighted_labels)\n",
    "    blood,blood_labels=sm3.fit_resample(blood,blood_labels)\n",
    "    ask,ask_labels=sm4.fit_resample(ask,ask_labels)\n",
    "    return result,result_labels,weighted,weighted_labels,blood,blood_labels,ask,ask_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "label='ADRCount'\n",
    "asklabel='ADRCount'\n",
    "\n",
    "#from collections import Counter\n",
    "#X_res, y_res = sm.fit_resample(result, result_labels)\n",
    "#print(Counter(result_labels))\n",
    "#print(Counter(y_res))\n",
    "def prepareDatasetNoSmote(label,asklab):\n",
    "    \n",
    "    result=pd.read_excel('result_reduced.xlsx')\n",
    "    weighted=pd.read_excel('Weighted_reduced.xlsx')\n",
    "    blood=pd.read_excel('blood_reduced.xlsx')\n",
    "    ask=pd.read_excel('askapatient_reduced.xlsx')\n",
    "\n",
    "    resultlabel=pd.read_excel('resultlabels.xlsx')\n",
    "    weightedlabel=pd.read_excel('Weightedlabels.xlsx')\n",
    "    bloodlabel=pd.read_excel('bloodlabels.xlsx')\n",
    "    asklabel=pd.read_excel('asklabels.xlsx')\n",
    "\n",
    "    result,result_labels=manipulate(result,resultlabel,label)\n",
    "    weighted,weighted_labels=manipulate(weighted,weightedlabel,label)\n",
    "    blood,blood_labels=manipulate(blood,bloodlabel,label)\n",
    "    ask,ask_labels=manipulate(ask,asklabel,asklab)\n",
    "    if 'Count' in label:\n",
    "        result_labels = vectorizeCount(result_labels)\n",
    "        weighted_labels = vectorizeCount(weighted_labels)\n",
    "        blood_labels=vectorizeCount(blood_labels)\n",
    "    if 'Count' in asklab:\n",
    "        ask_labels = vectorizeCount(ask_labels)\n",
    "    del result['weights2']\n",
    "    del result['Height']\n",
    "    \n",
    "    \n",
    "    #Applying SMOTENC\n",
    "    #result,result_labels=sm.fit_resample(result,result_labels)\n",
    "    #weighted,weighted_labels=sm.fit_resample(weighted,weighted_labels)\n",
    "    #blood,blood_labels=sm.fit_resample(blood,blood_labels)\n",
    "    #ask,ask_labels=sm.fit_resample(ask,ask_labels)\n",
    "    return result,result_labels,weighted,weighted_labels,blood,blood_labels,ask,ask_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for i in weighted:\\n    if i not in result:\\n        if i != 'weights2':\\n            del weighted[i]\\nweighted.to_excel('Weighted.xlsx')\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for i in weighted:\n",
    "    if i not in result:\n",
    "        if i != 'weights2':\n",
    "            del weighted[i]\n",
    "weighted.to_excel('Weighted.xlsx')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ask_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for i in weighted:\\n    print (weighted[i])'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for i in weighted:\n",
    "    print (weighted[i])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(model, X_train,X_test,Y_train,Y_test):\n",
    "    model.fit(X_train,Y_train)\n",
    "    return model.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-Fold Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#cross_val_score(RandomForestClassifier(n_estimators=1000),result,result_labels,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This was used for some other no longer existing purposes\n",
    "def getMissingPercentage(feature):\n",
    "    #nancount = int(result[result[feature].isnull()][feature].shape[0])\n",
    "    nancount = int(result[result[feature]==1][feature].shape[0])\n",
    "    size=int(result.shape[0])\n",
    "    print (nancount)\n",
    "    print ((nancount*100)/size)\n",
    "#getMissingPercentage('Pvc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.model_selection import KFold\\nfrom sklearn.ensemble import RandomForestClassifier\\nkf = KFold(n_splits=10)\\n\\nscores_rf=[]\\n\\nfor train_index, test_index in kf.split(result):\\n    X_train, X_test, Y_train, Y_test = result.iloc[train_index], result.iloc[test_index], result_labels.iloc[train_index],result_labels.iloc[test_index]\\n    scores_rf.append(get_score(RandomForestClassifier(n_estimators=400),X_train, X_test, Y_train, Y_test))'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "scores_rf=[]\n",
    "\n",
    "for train_index, test_index in kf.split(result):\n",
    "    X_train, X_test, Y_train, Y_test = result.iloc[train_index], result.iloc[test_index], result_labels.iloc[train_index],result_labels.iloc[test_index]\n",
    "    scores_rf.append(get_score(RandomForestClassifier(n_estimators=400),X_train, X_test, Y_train, Y_test))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomForest(result,result_labels,estimators,DataSet):\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from collections import Counter\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(result, result_labels, test_size=0.30, random_state=42)\n",
    "    print(Counter(result_labels))\n",
    "    clf = RandomForestClassifier(n_estimators=estimators,random_state=0)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    pred=clf.predict(X_test)\n",
    "    accuracy=(accuracy_score(Y_test, pred))\n",
    "    #print (accuracy)\n",
    "    scores_rf=(precision_recall_fscore_support(Y_test,pred, average='macro'))\n",
    "    #print(DataSet)\n",
    "    #print (pred)\n",
    "    #print(classification_report(Y_test,pred,labels=[0,1,2]))\n",
    "        \n",
    "    return accuracy,scores_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n",
      "Res: Accuracy: 0.5897435897435898 # Scores: (0.5911665646050215, 0.5870614035087719, 0.5837841843919069, None)\n",
      "Wei: Accuracy: 0.7435897435897436 # Scores: (0.6923076923076923, 0.711038961038961, 0.6990740740740741, None)\n",
      "Bld: Accuracy: 0.7122302158273381 # Scores: (0.6974110032362459, 0.6618037135278514, 0.667464114832536, None)\n",
      "Ask: Accuracy: 0.7709251101321586 # Scores: (0.5207920792079208, 0.5149359886201992, 0.5143186306780777, None)\n",
      "200\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n",
      "Res: Accuracy: 0.5918803418803419 # Scores: (0.592472915685351, 0.5896929824561403, 0.5877222516892282, None)\n",
      "Wei: Accuracy: 0.7692307692307693 # Scores: (0.7257142857142858, 0.7564935064935066, 0.7350943396226415, None)\n",
      "Bld: Accuracy: 0.697841726618705 # Scores: (0.6786677454153183, 0.6464412024756852, 0.6508373205741627, None)\n",
      "Ask: Accuracy: 0.7709251101321586 # Scores: (0.5207920792079208, 0.5149359886201992, 0.5143186306780777, None)\n",
      "300\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n",
      "Res: Accuracy: 0.5897435897435898 # Scores: (0.5905338886785308, 0.5873903508771929, 0.5850067429014798, None)\n",
      "Wei: Accuracy: 0.8205128205128205 # Scores: (0.7777777777777777, 0.7922077922077921, 0.784189723320158, None)\n",
      "Bld: Accuracy: 0.6906474820143885 # Scores: (0.6686539480657128, 0.6406940760389036, 0.6446703525355211, None)\n",
      "Ask: Accuracy: 0.7709251101321586 # Scores: (0.48890429958391124, 0.49317211948790896, 0.48606757227446884, None)\n",
      "400\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n",
      "Res: Accuracy: 0.5876068376068376 # Scores: (0.5880640602920395, 0.5854166666666667, 0.5834052072042986, None)\n",
      "Wei: Accuracy: 0.8205128205128205 # Scores: (0.7777777777777777, 0.7922077922077921, 0.784189723320158, None)\n",
      "Bld: Accuracy: 0.697841726618705 # Scores: (0.6786677454153183, 0.6464412024756852, 0.6508373205741627, None)\n",
      "Ask: Accuracy: 0.762114537444934 # Scores: (0.4983168316831683, 0.4987908961593172, 0.49563857801184985, None)\n",
      "500\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n",
      "Res: Accuracy: 0.5982905982905983 # Scores: (0.5998627316403569, 0.5957236842105262, 0.5928702315508911, None)\n",
      "Wei: Accuracy: 0.8205128205128205 # Scores: (0.7777777777777777, 0.7922077922077921, 0.784189723320158, None)\n",
      "Bld: Accuracy: 0.7050359712230215 # Scores: (0.6870694223635401, 0.6560565870910698, 0.6611973128827061, None)\n",
      "Ask: Accuracy: 0.7577092511013216 # Scores: (0.49483352468427094, 0.4961593172119488, 0.4931595826736492, None)\n",
      "600\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n",
      "Res: Accuracy: 0.5897435897435898 # Scores: (0.5905338886785308, 0.5873903508771929, 0.5850067429014798, None)\n",
      "Wei: Accuracy: 0.8205128205128205 # Scores: (0.7777777777777777, 0.7922077922077921, 0.784189723320158, None)\n",
      "Bld: Accuracy: 0.697841726618705 # Scores: (0.6771756122980719, 0.6503094606542883, 0.6549645390070922, None)\n",
      "Ask: Accuracy: 0.7709251101321586 # Scores: (0.48890429958391124, 0.49317211948790896, 0.48606757227446884, None)\n",
      "700\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n",
      "Res: Accuracy: 0.5897435897435898 # Scores: (0.5903532608695652, 0.5875, 0.5853820598006645, None)\n",
      "Wei: Accuracy: 0.8205128205128205 # Scores: (0.7777777777777777, 0.7922077922077921, 0.784189723320158, None)\n",
      "Bld: Accuracy: 0.7122302158273381 # Scores: (0.6974110032362459, 0.6618037135278514, 0.667464114832536, None)\n",
      "Ask: Accuracy: 0.7709251101321586 # Scores: (0.48890429958391124, 0.49317211948790896, 0.48606757227446884, None)\n",
      "800\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n",
      "Res: Accuracy: 0.5897435897435898 # Scores: (0.5903532608695652, 0.5875, 0.5853820598006645, None)\n",
      "Wei: Accuracy: 0.7948717948717948 # Scores: (0.75, 0.7743506493506493, 0.7592592592592593, None)\n",
      "Bld: Accuracy: 0.7050359712230215 # Scores: (0.6891483516483516, 0.6521883289124668, 0.6570379731600168, None)\n",
      "Ask: Accuracy: 0.7797356828193832 # Scores: (0.4972165991902834, 0.4984352773826458, 0.49075735821966976, None)\n",
      "900\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n",
      "Res: Accuracy: 0.5918803418803419 # Scores: (0.5928486089776412, 0.5894736842105264, 0.5869755063001622, None)\n",
      "Wei: Accuracy: 0.7948717948717948 # Scores: (0.75, 0.7743506493506493, 0.7592592592592593, None)\n",
      "Bld: Accuracy: 0.7122302158273381 # Scores: (0.6974110032362459, 0.6618037135278514, 0.667464114832536, None)\n",
      "Ask: Accuracy: 0.775330396475771 # Scores: (0.49287439613526574, 0.4958036984352774, 0.4883998409121039, None)\n",
      "1000\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n",
      "Res: Accuracy: 0.5918803418803419 # Scores: (0.5928486089776412, 0.5894736842105264, 0.5869755063001622, None)\n",
      "Wei: Accuracy: 0.8205128205128205 # Scores: (0.7777777777777777, 0.7922077922077921, 0.784189723320158, None)\n",
      "Bld: Accuracy: 0.7122302158273381 # Scores: (0.6974110032362459, 0.6618037135278514, 0.667464114832536, None)\n",
      "Ask: Accuracy: 0.7797356828193832 # Scores: (0.4754901960784314, 0.4875533428165007, 0.47453703703703703, None)\n",
      "1100\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n",
      "Res: Accuracy: 0.5918803418803419 # Scores: (0.5928486089776412, 0.5894736842105264, 0.5869755063001622, None)\n",
      "Wei: Accuracy: 0.8205128205128205 # Scores: (0.7777777777777777, 0.7922077922077921, 0.784189723320158, None)\n",
      "Bld: Accuracy: 0.7050359712230215 # Scores: (0.6891483516483516, 0.6521883289124668, 0.6570379731600168, None)\n",
      "Ask: Accuracy: 0.7841409691629956 # Scores: (0.4512072434607646, 0.479302987197724, 0.45881379847224246, None)\n",
      "1200\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n",
      "Res: Accuracy: 0.5897435897435898 # Scores: (0.5905338886785308, 0.5873903508771929, 0.5850067429014798, None)\n",
      "Wei: Accuracy: 0.8205128205128205 # Scores: (0.7777777777777777, 0.7922077922077921, 0.784189723320158, None)\n",
      "Bld: Accuracy: 0.7050359712230215 # Scores: (0.6891483516483516, 0.6521883289124668, 0.6570379731600168, None)\n",
      "Ask: Accuracy: 0.775330396475771 # Scores: (0.4718234981392876, 0.48492176386913227, 0.4724538619275462, None)\n",
      "1300\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n",
      "Res: Accuracy: 0.5897435897435898 # Scores: (0.5905338886785308, 0.5873903508771929, 0.5850067429014798, None)\n",
      "Wei: Accuracy: 0.8205128205128205 # Scores: (0.7777777777777777, 0.7922077922077921, 0.784189723320158, None)\n",
      "Bld: Accuracy: 0.7122302158273381 # Scores: (0.6974110032362459, 0.6618037135278514, 0.667464114832536, None)\n",
      "Ask: Accuracy: 0.7841409691629956 # Scores: (0.47956161137440756, 0.49018492176386913, 0.4766385921987484, None)\n",
      "1400\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n",
      "Res: Accuracy: 0.5897435897435898 # Scores: (0.5905338886785308, 0.5873903508771929, 0.5850067429014798, None)\n",
      "Wei: Accuracy: 0.8205128205128205 # Scores: (0.7777777777777777, 0.7922077922077921, 0.784189723320158, None)\n",
      "Bld: Accuracy: 0.7194244604316546 # Scores: (0.7082417582417582, 0.667550839964633, 0.673767828127821, None)\n",
      "Ask: Accuracy: 0.7841409691629956 # Scores: (0.47956161137440756, 0.49018492176386913, 0.4766385921987484, None)\n",
      "1500\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n",
      "Res: Accuracy: 0.5876068376068376 # Scores: (0.5880640602920395, 0.5854166666666667, 0.5834052072042986, None)\n",
      "Wei: Accuracy: 0.8205128205128205 # Scores: (0.7777777777777777, 0.7922077922077921, 0.784189723320158, None)\n",
      "Bld: Accuracy: 0.7122302158273381 # Scores: (0.7001400560224089, 0.6579354553492485, 0.6632751937984496, None)\n",
      "Ask: Accuracy: 0.775330396475771 # Scores: (0.4718234981392876, 0.48492176386913227, 0.4724538619275462, None)\n",
      "1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n",
      "Res: Accuracy: 0.5854700854700855 # Scores: (0.5859375, 0.5832236842105263, 0.5810631229235881, None)\n",
      "Wei: Accuracy: 0.8205128205128205 # Scores: (0.7777777777777777, 0.7922077922077921, 0.784189723320158, None)\n",
      "Bld: Accuracy: 0.7122302158273381 # Scores: (0.7001400560224089, 0.6579354553492485, 0.6632751937984496, None)\n",
      "Ask: Accuracy: 0.775330396475771 # Scores: (0.4718234981392876, 0.48492176386913227, 0.4724538619275462, None)\n",
      "1700\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n",
      "Res: Accuracy: 0.5876068376068376 # Scores: (0.5886063904694845, 0.5850877192982457, 0.5822491501514695, None)\n",
      "Wei: Accuracy: 0.8205128205128205 # Scores: (0.7777777777777777, 0.7922077922077921, 0.784189723320158, None)\n",
      "Bld: Accuracy: 0.7122302158273381 # Scores: (0.7001400560224089, 0.6579354553492485, 0.6632751937984496, None)\n",
      "Ask: Accuracy: 0.775330396475771 # Scores: (0.49287439613526574, 0.4958036984352774, 0.4883998409121039, None)\n",
      "1800\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n",
      "Res: Accuracy: 0.5876068376068376 # Scores: (0.5882302908877842, 0.5853070175438597, 0.5830359657100123, None)\n",
      "Wei: Accuracy: 0.8205128205128205 # Scores: (0.7777777777777777, 0.7922077922077921, 0.784189723320158, None)\n",
      "Bld: Accuracy: 0.7122302158273381 # Scores: (0.7001400560224089, 0.6579354553492485, 0.6632751937984496, None)\n",
      "Ask: Accuracy: 0.7709251101321586 # Scores: (0.48890429958391124, 0.49317211948790896, 0.48606757227446884, None)\n",
      "1900\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n",
      "Res: Accuracy: 0.5876068376068376 # Scores: (0.5882302908877842, 0.5853070175438597, 0.5830359657100123, None)\n",
      "Wei: Accuracy: 0.8205128205128205 # Scores: (0.7777777777777777, 0.7922077922077921, 0.784189723320158, None)\n",
      "Bld: Accuracy: 0.7122302158273381 # Scores: (0.7001400560224089, 0.6579354553492485, 0.6632751937984496, None)\n",
      "Ask: Accuracy: 0.7709251101321586 # Scores: (0.48890429958391124, 0.49317211948790896, 0.48606757227446884, None)\n"
     ]
    }
   ],
   "source": [
    "label='Hypertensive disease '\n",
    "asklabel='Hypertensive disease '\n",
    "res_Stats = pd.DataFrame(columns = ['Estimators' , 'Accuracy', 'Precision' , 'Recall','FScore'])\n",
    "wei_Stats = pd.DataFrame(columns = ['Estimators' , 'Accuracy', 'Precision' , 'Recall','FScore'])\n",
    "bld_Stats = pd.DataFrame(columns = ['Estimators' , 'Accuracy', 'Precision' , 'Recall','FScore'])\n",
    "ask_Stats = pd.DataFrame(columns = ['Estimators' , 'Accuracy', 'Precision' , 'Recall','FScore'])\n",
    "for estimators in range (100,2000,100):\n",
    "    print (estimators)\n",
    "    result,result_labels,weighted,weighted_labels,blood,blood_labels,ask,ask_labels=prepareDataset(label,asklabel)\n",
    "    res_acc,res_scores=randomForest(result,result_labels,estimators,\"Result\")\n",
    "    wei_acc,wei_scores=randomForest(weighted,weighted_labels,estimators,\"Weighted\")\n",
    "    blood_acc,blood_scores=randomForest(blood,blood_labels,estimators,\"Blood\")\n",
    "    ask_acc,ask_scores=randomForest(ask,ask_labels,estimators,\"Ask\")\n",
    "    print(\"Res: Accuracy: \" + str(res_acc)+\" # \"+ \"Scores: \"+str(res_scores))\n",
    "    print(\"Wei: Accuracy: \" + str(wei_acc)+\" # \"+ \"Scores: \"+str(wei_scores))\n",
    "    print(\"Bld: Accuracy: \" + str(blood_acc)+\" # \"+ \"Scores: \"+str(blood_scores))\n",
    "    print(\"Ask: Accuracy: \" + str(ask_acc)+\" # \"+ \"Scores: \"+str(ask_scores))\n",
    "    res_Stats=res_Stats.append({'Estimators':estimators,'Accuracy':res_acc,'Precision':res_scores[0],'Recall':res_scores[1],'FScore':res_scores[2]},ignore_index=True)\n",
    "    wei_Stats=wei_Stats.append({'Estimators':estimators,'Accuracy':wei_acc,'Precision':wei_scores[0],'Recall':wei_scores[1],'FScore':wei_scores[2]},ignore_index=True)\n",
    "    bld_Stats=bld_Stats.append({'Estimators':estimators,'Accuracy':blood_acc,'Precision':blood_scores[0],'Recall':blood_scores[1],'FScore':blood_scores[2]},ignore_index=True)\n",
    "    ask_Stats=ask_Stats.append({'Estimators':estimators,'Accuracy':ask_acc,'Precision':ask_scores[0],'Recall':ask_scores[1],'FScore':ask_scores[2]},ignore_index=True)\n",
    "res_Stats.to_excel('Forrestresult_Stats'+label+'smoteless.xlsx') \n",
    "wei_Stats.to_excel('Forrestweight_Stats'+label+'smoteless.xlsx') \n",
    "bld_Stats.to_excel('Forrestblood_Stats'+label+'smoteless.xlsx') \n",
    "ask_Stats.to_excel('Forrestask_Stats'+asklabel+'smoteless.xlsx') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier Function 10-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average(scores):\n",
    "    FScore=0\n",
    "    Recall=0\n",
    "    Precision=0\n",
    "\n",
    "    for i in scores:\n",
    "        Precision+=i[0]\n",
    "        Recall+=i[1]\n",
    "        FScore+=i[2]\n",
    "    return (Precision/10,Recall/10,FScore/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomForest10Fold(result,result_labels,estimators,DataSet):\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from collections import Counter\n",
    "    kf = KFold(n_splits=10)\n",
    "\n",
    "    accuracy=[]\n",
    "\n",
    "    scores_rf=[]\n",
    "    print(Counter(result_labels))\n",
    "    count = 1\n",
    "    for train_index, test_index in kf.split(result):\n",
    "        X_train, X_test, Y_train, Y_test = result[train_index], result[test_index], result_labels[train_index],result_labels[test_index]\n",
    "        clf = RandomForestClassifier(n_estimators=estimators, max_depth=2,random_state=0)\n",
    "        clf.fit(X_train, Y_train)\n",
    "        pred=clf.predict(X_test)\n",
    "        accuracy.append(accuracy_score(Y_test, pred))\n",
    "        scores_rf.append(precision_recall_fscore_support(Y_test,pred, average='macro'))\n",
    "        \n",
    "        #print(str(count)+DataSet)\n",
    "        #print(f1_score(Y_test, pred, average='micro'))\n",
    "        count+=1\n",
    "        #print (pred)\n",
    "        #print(classification_report(Y_test,pred,labels=[0,1]))\n",
    "    accuracy = np.mean(accuracy,axis=0)   \n",
    "    return accuracy,average(scores_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest with 10-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.2137214758925285 # Scores: (0.2137214758925285, 0.2137214758925285, 0.2137214758925285)\n",
      "Wei: Accuracy: 0.2757142857142857 # Scores: (0.2757142857142857, 0.2757142857142857, 0.2757142857142857)\n",
      "Bld: Accuracy: 0.29458653026427967 # Scores: (0.2945865302642796, 0.2945865302642796, 0.2945865302642796)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.20939915347810087 # Scores: (0.20939915347810084, 0.20939915347810084, 0.20939915347810084)\n",
      "Wei: Accuracy: 0.30476190476190473 # Scores: (0.3047619047619048, 0.3047619047619048, 0.3047619047619048)\n",
      "Bld: Accuracy: 0.30328218243819266 # Scores: (0.3032821824381927, 0.3032821824381927, 0.3032821824381927)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.21083456017666546 # Scores: (0.21083456017666546, 0.21083456017666546, 0.21083456017666546)\n",
      "Wei: Accuracy: 0.2914285714285714 # Scores: (0.2914285714285715, 0.2914285714285715, 0.2914285714285715)\n",
      "Bld: Accuracy: 0.3018329070758738 # Scores: (0.30183290707587385, 0.30183290707587385, 0.30183290707587385)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.21083456017666546 # Scores: (0.21083456017666546, 0.21083456017666546, 0.21083456017666546)\n",
      "Wei: Accuracy: 0.2780952380952381 # Scores: (0.2780952380952381, 0.2780952380952381, 0.2780952380952381)\n",
      "Bld: Accuracy: 0.2959931798806479 # Scores: (0.2959931798806479, 0.2959931798806479, 0.2959931798806479)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.21083456017666546 # Scores: (0.21083456017666546, 0.21083456017666546, 0.21083456017666546)\n",
      "Wei: Accuracy: 0.2857142857142857 # Scores: (0.2857142857142857, 0.2857142857142857, 0.2857142857142857)\n",
      "Bld: Accuracy: 0.2930946291560102 # Scores: (0.2930946291560102, 0.2930946291560102, 0.2930946291560102)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.21083456017666546 # Scores: (0.21083456017666546, 0.21083456017666546, 0.21083456017666546)\n",
      "Wei: Accuracy: 0.28523809523809524 # Scores: (0.28523809523809524, 0.28523809523809524, 0.28523809523809524)\n",
      "Bld: Accuracy: 0.2959931798806479 # Scores: (0.2959931798806479, 0.2959931798806479, 0.2959931798806479)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.20939685314685316 # Scores: (0.20939685314685313, 0.20939685314685313, 0.20939685314685313)\n",
      "Wei: Accuracy: 0.2985714285714286 # Scores: (0.2985714285714286, 0.2985714285714286, 0.2985714285714286)\n",
      "Bld: Accuracy: 0.2945865302642796 # Scores: (0.2945865302642796, 0.2945865302642796, 0.2945865302642796)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.2098753220463747 # Scores: (0.20987532204637466, 0.20987532204637466, 0.20987532204637466)\n",
      "Wei: Accuracy: 0.2842857142857143 # Scores: (0.2842857142857143, 0.2842857142857143, 0.2842857142857143)\n",
      "Bld: Accuracy: 0.2930946291560102 # Scores: (0.2930946291560102, 0.2930946291560102, 0.2930946291560102)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.2098753220463747 # Scores: (0.20987532204637466, 0.20987532204637466, 0.20987532204637466)\n",
      "Wei: Accuracy: 0.28476190476190477 # Scores: (0.28476190476190477, 0.28476190476190477, 0.28476190476190477)\n",
      "Bld: Accuracy: 0.29744245524296675 # Scores: (0.29744245524296675, 0.29744245524296675, 0.29744245524296675)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.21035609127714391 # Scores: (0.21035609127714391, 0.21035609127714391, 0.21035609127714391)\n",
      "Wei: Accuracy: 0.2771428571428572 # Scores: (0.2771428571428572, 0.2771428571428572, 0.2771428571428572)\n",
      "Bld: Accuracy: 0.29744245524296675 # Scores: (0.29744245524296675, 0.29744245524296675, 0.29744245524296675)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.20892068457857932 # Scores: (0.20892068457857932, 0.20892068457857932, 0.20892068457857932)\n",
      "Wei: Accuracy: 0.28380952380952384 # Scores: (0.28380952380952384, 0.28380952380952384, 0.28380952380952384)\n",
      "Bld: Accuracy: 0.2989343563512361 # Scores: (0.29893435635123616, 0.29893435635123616, 0.29893435635123616)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.2108345601766654 # Scores: (0.21083456017666546, 0.21083456017666546, 0.21083456017666546)\n",
      "Wei: Accuracy: 0.2771428571428572 # Scores: (0.2771428571428572, 0.2771428571428572, 0.2771428571428572)\n",
      "Bld: Accuracy: 0.300383631713555 # Scores: (0.300383631713555, 0.300383631713555, 0.300383631713555)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.21035609127714391 # Scores: (0.21035609127714391, 0.21035609127714391, 0.21035609127714391)\n",
      "Wei: Accuracy: 0.29095238095238096 # Scores: (0.29095238095238096, 0.29095238095238096, 0.29095238095238096)\n",
      "Bld: Accuracy: 0.297463768115942 # Scores: (0.297463768115942, 0.297463768115942, 0.297463768115942)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.21035609127714391 # Scores: (0.21035609127714391, 0.21035609127714391, 0.21035609127714391)\n",
      "Wei: Accuracy: 0.29809523809523814 # Scores: (0.2980952380952381, 0.2980952380952381, 0.29809523809523814)\n",
      "Bld: Accuracy: 0.2989343563512361 # Scores: (0.2989343563512361, 0.2989343563512361, 0.2989343563512361)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.21035609127714391 # Scores: (0.21035609127714391, 0.21035609127714391, 0.21035609127714391)\n",
      "Wei: Accuracy: 0.29809523809523814 # Scores: (0.2980952380952381, 0.2980952380952381, 0.29809523809523814)\n",
      "Bld: Accuracy: 0.300383631713555 # Scores: (0.300383631713555, 0.300383631713555, 0.300383631713555)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.2108345601766654 # Scores: (0.21083456017666546, 0.21083456017666546, 0.21083456017666546)\n",
      "Wei: Accuracy: 0.2914285714285715 # Scores: (0.2914285714285715, 0.2914285714285715, 0.2914285714285715)\n",
      "Bld: Accuracy: 0.3018329070758738 # Scores: (0.3018329070758738, 0.3018329070758738, 0.3018329070758738)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.2108345601766654 # Scores: (0.21083456017666546, 0.21083456017666546, 0.21083456017666546)\n",
      "Wei: Accuracy: 0.2842857142857143 # Scores: (0.28428571428571436, 0.28428571428571436, 0.28428571428571436)\n",
      "Bld: Accuracy: 0.29889173060528557 # Scores: (0.29889173060528557, 0.29889173060528557, 0.29889173060528557)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.21035609127714391 # Scores: (0.21035609127714391, 0.21035609127714391, 0.21035609127714391)\n",
      "Wei: Accuracy: 0.29095238095238096 # Scores: (0.29095238095238096, 0.29095238095238096, 0.29095238095238096)\n",
      "Bld: Accuracy: 0.29889173060528557 # Scores: (0.29889173060528557, 0.29889173060528557, 0.29889173060528557)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.2084422156790578 # Scores: (0.2084422156790578, 0.2084422156790578, 0.2084422156790578)\n",
      "Wei: Accuracy: 0.29095238095238096 # Scores: (0.29095238095238096, 0.29095238095238096, 0.29095238095238096)\n",
      "Bld: Accuracy: 0.29744245524296675 # Scores: (0.29744245524296675, 0.29744245524296675, 0.29744245524296675)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n"
     ]
    }
   ],
   "source": [
    "label='ADRCount'\n",
    "asklabel='ADRCount'\n",
    "\n",
    "res_Stats = pd.DataFrame(columns = ['Estimators' , 'Accuracy', 'Precision' , 'Recall','FScore'])\n",
    "wei_Stats = pd.DataFrame(columns = ['Estimators' , 'Accuracy', 'Precision' , 'Recall','FScore'])\n",
    "bld_Stats = pd.DataFrame(columns = ['Estimators' , 'Accuracy', 'Precision' , 'Recall','FScore'])\n",
    "ask_Stats = pd.DataFrame(columns = ['Estimators' , 'Accuracy', 'Precision' , 'Recall','FScore'])\n",
    "for estimators in range (100,2000,100):\n",
    "    result,result_labels,weighted,weighted_labels,blood,blood_labels,ask,ask_labels=prepareDataset(label,asklabel)\n",
    "    res_acc,res_scores=randomForest10Fold(result,result_labels,estimators,\"Result\")\n",
    "    wei_acc,wei_scores=randomForest10Fold(weighted,weighted_labels,estimators,\"Weighted\")\n",
    "    blood_acc,blood_scores=randomForest10Fold(blood,blood_labels,estimators,\"Blood\")\n",
    "    ask_acc,ask_scores=randomForest10Fold(ask,ask_labels,1000,\"Ask\")\n",
    "    print(\"Res: Accuracy: \" + str(res_acc)+\" # \"+ \"Scores: \"+str(res_scores))\n",
    "    print(\"Wei: Accuracy: \" + str(wei_acc)+\" # \"+ \"Scores: \"+str(wei_scores))\n",
    "    print(\"Bld: Accuracy: \" + str(blood_acc)+\" # \"+ \"Scores: \"+str(blood_scores))\n",
    "    print(\"Ask: Accuracy: \" + str(ask_acc)+\" # \"+ \"Scores: \"+str(ask_scores))\n",
    "    res_Stats=res_Stats.append({'Estimators':estimators,'Accuracy':res_acc,'Precision':res_scores[0],'Recall':res_scores[1],'FScore':res_scores[2]},ignore_index=True)\n",
    "    wei_Stats=wei_Stats.append({'Estimators':estimators,'Accuracy':wei_acc,'Precision':wei_scores[0],'Recall':wei_scores[1],'FScore':wei_scores[2]},ignore_index=True)\n",
    "    bld_Stats=bld_Stats.append({'Estimators':estimators,'Accuracy':blood_acc,'Precision':blood_scores[0],'Recall':blood_scores[1],'FScore':blood_scores[2]},ignore_index=True)\n",
    "    ask_Stats=ask_Stats.append({'Estimators':estimators,'Accuracy':ask_acc,'Precision':ask_scores[0],'Recall':ask_scores[1],'FScore':ask_scores[2]},ignore_index=True)\n",
    "res_Stats.to_excel('ForrestFoldresult_Stats'+label+'.xlsx') \n",
    "wei_Stats.to_excel('ForrestFoldweight_Stats'+label+'.xlsx') \n",
    "bld_Stats.to_excel('ForrestFoldblood_Stats'+label+'.xlsx') \n",
    "ask_Stats.to_excel('ForrestFoldask_Stats'+asklabel+'.xlsx') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ask_scores[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Classifier Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(result,result_labels,DataSet):\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from collections import Counter\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(result, result_labels, test_size=0.30, random_state=42)\n",
    "    print(Counter(result_labels))\n",
    "    clf = SVC(gamma='auto')\n",
    "    clf.fit(X_train, Y_train)\n",
    "    pred=clf.predict(X_test)\n",
    "    accuracy=(accuracy_score(Y_test, pred))\n",
    "    #print (accuracy)\n",
    "    scores_rf=(precision_recall_fscore_support(Y_test,pred, average='macro'))\n",
    "    #print(DataSet)\n",
    "    #print (pred)\n",
    "    #print(classification_report(Y_test,pred,labels=[0,1,2]))\n",
    "        \n",
    "    return accuracy,scores_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Atenolol': 620, 'Lisinopril': 428, 'Amlodipine': 161, 'Hydrochlorothiazide': 129, 'Nadolol': 119, 'Diltiazem': 100})\n",
      "Counter({'Lisinopril': 51, 'Atenolol': 45, 'Amlodipine': 14, 'Hydrochlorothiazide': 13})\n",
      "Counter({'Atenolol': 186, 'Lisinopril': 134, 'Amlodipine': 63, 'Hydrochlorothiazide': 51, 'Diltiazem': 26})\n",
      "Counter({'Lisinopril': 622, 'Atenolol': 134})\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "[1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "[1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "Counter({0: 620, 2: 620, 3: 620, 1: 620, 4: 620, 5: 620})\n",
      "Counter({0: 51, 2: 51, 1: 51, 3: 51})\n",
      "Counter({0: 186, 2: 186, 3: 186, 1: 186, 4: 186})\n",
      "Counter({1: 622, 0: 622})\n",
      "Res: Accuracy: 0.43727598566308246 # Scores: (0.4408465002376966, 0.43831468465976026, 0.43491665821041336, None)\n",
      "Wei: Accuracy: 0.5161290322580645 # Scores: (0.5428685897435898, 0.5167279411764706, 0.5243712695325597, None)\n",
      "Bld: Accuracy: 0.5232974910394266 # Scores: (0.5215450761447806, 0.5306530214424952, 0.5201889935353436, None)\n",
      "Ask: Accuracy: 0.7566844919786097 # Scores: (0.76807728781413, 0.7588046805710525, 0.7550012597631646, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"res_Stats.to_excel('Forrestresult_Stats'+label+'smoteless.xlsx') \\nwei_Stats.to_excel('Forrestweight_Stats'+label+'smoteless.xlsx') \\nbld_Stats.to_excel('Forrestblood_Stats'+label+'smoteless.xlsx') \\nask_Stats.to_excel('Forrestask_Stats'+asklabel+'smoteless.xlsx') \""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label='Drug'\n",
    "asklabel='Drug'\n",
    "Stats = pd.DataFrame(columns = ['Dataset','Accuracy', 'Precision' , 'Recall','FScore'])\n",
    "\n",
    "\n",
    "result,result_labels,weighted,weighted_labels,blood,blood_labels,ask,ask_labels=prepareDataset(label,asklabel)\n",
    "res_acc,res_scores=SVM(result,result_labels,\"Result\")\n",
    "wei_acc,wei_scores=SVM(weighted,weighted_labels,\"Weighted\")\n",
    "blood_acc,blood_scores=SVM(blood,blood_labels,\"Blood\")\n",
    "ask_acc,ask_scores=SVM(ask,ask_labels,\"Ask\")\n",
    "print(\"Res: Accuracy: \" + str(res_acc)+\" # \"+ \"Scores: \"+str(res_scores))\n",
    "print(\"Wei: Accuracy: \" + str(wei_acc)+\" # \"+ \"Scores: \"+str(wei_scores))\n",
    "print(\"Bld: Accuracy: \" + str(blood_acc)+\" # \"+ \"Scores: \"+str(blood_scores))\n",
    "print(\"Ask: Accuracy: \" + str(ask_acc)+\" # \"+ \"Scores: \"+str(ask_scores))\n",
    "Stats=Stats.append({'Dataset':\"Complete\",'Accuracy':res_acc,'Precision':res_scores[0],'Recall':res_scores[1],'FScore':res_scores[2]},ignore_index=True)\n",
    "Stats=Stats.append({'Dataset':\"Weighted\",'Accuracy':wei_acc,'Precision':wei_scores[0],'Recall':wei_scores[1],'FScore':wei_scores[2]},ignore_index=True)\n",
    "Stats=Stats.append({'Dataset':\"Blood\",'Accuracy':blood_acc,'Precision':blood_scores[0],'Recall':blood_scores[1],'FScore':blood_scores[2]},ignore_index=True)\n",
    "Stats=Stats.append({'Dataset':\"AskAPatient\",'Accuracy':ask_acc,'Precision':ask_scores[0],'Recall':ask_scores[1],'FScore':ask_scores[2]},ignore_index=True)\n",
    "Stats.to_excel('SVMresult_Stats'+label+'.xlsx') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Atenolol': 620, 'Lisinopril': 428, 'Amlodipine': 161, 'Hydrochlorothiazide': 129, 'Nadolol': 119, 'Diltiazem': 100})\n",
      "Counter({'Lisinopril': 51, 'Atenolol': 45, 'Amlodipine': 14, 'Hydrochlorothiazide': 13})\n",
      "Counter({'Atenolol': 186, 'Lisinopril': 134, 'Amlodipine': 63, 'Hydrochlorothiazide': 51, 'Diltiazem': 26})\n",
      "Counter({'Lisinopril': 622, 'Atenolol': 134})\n",
      "Counter({1: 620, 4: 428, 0: 161, 3: 129, 5: 119, 2: 100})\n",
      "Counter({3: 51, 1: 45, 0: 14, 2: 13})\n",
      "Counter({1: 186, 4: 134, 0: 63, 3: 51, 2: 26})\n",
      "Counter({1: 622, 0: 134})\n",
      "Res: Accuracy: 0.4252136752136752 # Scores: (0.29880496259806605, 0.19900557302132105, 0.1690978918393249, None)\n",
      "Wei: Accuracy: 0.32432432432432434 # Scores: (0.1625, 0.20833333333333331, 0.175, None)\n",
      "Bld: Accuracy: 0.3115942028985507 # Scores: (0.24755266743527934, 0.2130346304259348, 0.18980392156862744, None)\n",
      "Ask: Accuracy: 0.8810572687224669 # Scores: (0.9402654867256637, 0.5178571428571429, 0.5027180527383367, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "label='Drug'\n",
    "asklabel='Drug'\n",
    "Stats = pd.DataFrame(columns = ['Dataset','Accuracy', 'Precision' , 'Recall','FScore'])\n",
    "\n",
    "\n",
    "result,result_labels,weighted,weighted_labels,blood,blood_labels,ask,ask_labels=prepareDatasetNoSmote(label,asklabel)\n",
    "res_acc,res_scores=SVM(result,result_labels,\"Result\")\n",
    "wei_acc,wei_scores=SVM(weighted,weighted_labels,\"Weighted\")\n",
    "blood_acc,blood_scores=SVM(blood,blood_labels,\"Blood\")\n",
    "ask_acc,ask_scores=SVM(ask,ask_labels,\"Ask\")\n",
    "print(\"Res: Accuracy: \" + str(res_acc)+\" # \"+ \"Scores: \"+str(res_scores))\n",
    "print(\"Wei: Accuracy: \" + str(wei_acc)+\" # \"+ \"Scores: \"+str(wei_scores))\n",
    "print(\"Bld: Accuracy: \" + str(blood_acc)+\" # \"+ \"Scores: \"+str(blood_scores))\n",
    "print(\"Ask: Accuracy: \" + str(ask_acc)+\" # \"+ \"Scores: \"+str(ask_scores))\n",
    "Stats=Stats.append({'Dataset':\"Complete\",'Accuracy':res_acc,'Precision':res_scores[0],'Recall':res_scores[1],'FScore':res_scores[2]},ignore_index=True)\n",
    "Stats=Stats.append({'Dataset':\"Weighted\",'Accuracy':wei_acc,'Precision':wei_scores[0],'Recall':wei_scores[1],'FScore':wei_scores[2]},ignore_index=True)\n",
    "Stats=Stats.append({'Dataset':\"Blood\",'Accuracy':blood_acc,'Precision':blood_scores[0],'Recall':blood_scores[1],'FScore':blood_scores[2]},ignore_index=True)\n",
    "Stats=Stats.append({'Dataset':\"AskAPatient\",'Accuracy':ask_acc,'Precision':ask_scores[0],'Recall':ask_scores[1],'FScore':ask_scores[2]},ignore_index=True)\n",
    "Stats.to_excel('SVMresult_Stats'+label+'smoteless.xlsx') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM 10-Fold Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM10Fold(result,result_labels,DataSet):\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from collections import Counter\n",
    "    kf = KFold(n_splits=10)\n",
    "\n",
    "    accuracy=[]\n",
    "\n",
    "    scores_rf=[]\n",
    "    print(Counter(result_labels))\n",
    "    count = 1\n",
    "    for train_index, test_index in kf.split(result):\n",
    "        X_train, X_test, Y_train, Y_test = result[train_index], result[test_index], result_labels[train_index],result_labels[test_index]\n",
    "        clf = SVC(gamma='auto')\n",
    "        clf.fit(X_train, Y_train)\n",
    "        pred=clf.predict(X_test)\n",
    "        accuracy.append(accuracy_score(Y_test, pred))\n",
    "        scores_rf.append(precision_recall_fscore_support(Y_test,pred, average='macro'))\n",
    "        \n",
    "        #print(str(count)+DataSet)\n",
    "        #print(f1_score(Y_test, pred, average='micro'))\n",
    "        count+=1\n",
    "        #print (pred)\n",
    "        #print(classification_report(Y_test,pred,labels=[0,1]))\n",
    "    accuracy = np.mean(accuracy,axis=0)   \n",
    "    return accuracy,average(scores_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM10FoldNoSmote(result,result_labels,DataSet):\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from collections import Counter\n",
    "    kf = KFold(n_splits=10)\n",
    "    print(result)\n",
    "    accuracy=[]\n",
    "\n",
    "    scores_rf=[]\n",
    "    print(Counter(result_labels))\n",
    "    count = 1\n",
    "    for train_index, test_index in kf.split(result):\n",
    "        X_train, X_test, Y_train, Y_test = result.iloc[train_index], result.iloc[test_index], result_labels.iloc[train_index],result_labels.iloc[test_index]\n",
    "        clf = SVC(gamma='auto')\n",
    "        clf.fit(X_train, Y_train)\n",
    "        pred=clf.predict(X_test)\n",
    "        accuracy.append(accuracy_score(Y_test, pred))\n",
    "        scores_rf.append(precision_recall_fscore_support(Y_test,pred, average='macro'))\n",
    "        \n",
    "        #print(str(count)+DataSet)\n",
    "        #print(f1_score(Y_test, pred, average='micro'))\n",
    "        count+=1\n",
    "        #print (pred)\n",
    "        #print(classification_report(Y_test,pred,labels=[0,1]))\n",
    "    accuracy = np.mean(accuracy,axis=0)   \n",
    "    return accuracy,average(scores_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM 10-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Atenolol': 620, 'Lisinopril': 428, 'Amlodipine': 161, 'Hydrochlorothiazide': 129, 'Nadolol': 119, 'Diltiazem': 100})\n",
      "Counter({'Lisinopril': 51, 'Atenolol': 45, 'Amlodipine': 14, 'Hydrochlorothiazide': 13})\n",
      "Counter({'Atenolol': 186, 'Lisinopril': 134, 'Amlodipine': 63, 'Hydrochlorothiazide': 51, 'Diltiazem': 26})\n",
      "Counter({'Lisinopril': 622, 'Atenolol': 134})\n",
      "Counter({0: 620, 2: 620, 3: 620, 1: 620, 4: 620, 5: 620})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 51, 2: 51, 1: 51, 3: 51})\n",
      "Counter({0: 186, 2: 186, 3: 186, 1: 186, 4: 186})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 622, 0: 622})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res: Accuracy: 0.18440860215053761 # Scores: (0.2182665943944818, 0.08541983140022649, 0.10079218319805756)\n",
      "Wei: Accuracy: 0.26976190476190476 # Scores: (0.23709249084249082, 0.139578373015873, 0.14684333189189974)\n",
      "Bld: Accuracy: 0.35268817204301073 # Scores: (0.2706027138288802, 0.1636906171871692, 0.17005666735821093)\n",
      "Ask: Accuracy: 0.7238709677419355 # Scores: (0.5038461538461538, 0.411776837652036, 0.42419609811645004)\n"
     ]
    }
   ],
   "source": [
    "Stats = pd.DataFrame(columns = ['Dataset','Accuracy', 'Precision' , 'Recall','FScore'])\n",
    "\n",
    "label='Drug'\n",
    "asklabel='Drug'\n",
    "result,result_labels,weighted,weighted_labels,blood,blood_labels,ask,ask_labels=prepareDataset(label,asklabel)\n",
    "res_acc,res_scores=SVM10Fold(result,result_labels,\"Result\")\n",
    "wei_acc,wei_scores=SVM10Fold(weighted,weighted_labels,\"Weighted\")\n",
    "blood_acc,blood_scores=SVM10Fold(blood,blood_labels,\"Blood\")\n",
    "ask_acc,ask_scores=SVM10Fold(ask,ask_labels,\"Ask\")\n",
    "print(\"Res: Accuracy: \" + str(res_acc)+\" # \"+ \"Scores: \"+str(res_scores))\n",
    "print(\"Wei: Accuracy: \" + str(wei_acc)+\" # \"+ \"Scores: \"+str(wei_scores))\n",
    "print(\"Bld: Accuracy: \" + str(blood_acc)+\" # \"+ \"Scores: \"+str(blood_scores))\n",
    "print(\"Ask: Accuracy: \" + str(ask_acc)+\" # \"+ \"Scores: \"+str(ask_scores))\n",
    "Stats=Stats.append({'Dataset':\"Complete\",'Accuracy':res_acc,'Precision':res_scores[0],'Recall':res_scores[1],'FScore':res_scores[2]},ignore_index=True)\n",
    "Stats=Stats.append({'Dataset':\"Weighted\",'Accuracy':wei_acc,'Precision':wei_scores[0],'Recall':wei_scores[1],'FScore':wei_scores[2]},ignore_index=True)\n",
    "Stats=Stats.append({'Dataset':\"Blood\",'Accuracy':blood_acc,'Precision':blood_scores[0],'Recall':blood_scores[1],'FScore':blood_scores[2]},ignore_index=True)\n",
    "Stats=Stats.append({'Dataset':\"AskAPatient\",'Accuracy':ask_acc,'Precision':ask_scores[0],'Recall':ask_scores[1],'FScore':ask_scores[2]},ignore_index=True)\n",
    "Stats.to_excel('SVMresult_Stats10Fold'+label+'.xlsx') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Atenolol': 620, 'Lisinopril': 428, 'Amlodipine': 161, 'Hydrochlorothiazide': 129, 'Nadolol': 119, 'Diltiazem': 100})\n",
      "Counter({'Lisinopril': 51, 'Atenolol': 45, 'Amlodipine': 14, 'Hydrochlorothiazide': 13})\n",
      "Counter({'Atenolol': 186, 'Lisinopril': 134, 'Amlodipine': 63, 'Hydrochlorothiazide': 51, 'Diltiazem': 26})\n",
      "Counter({'Lisinopril': 622, 'Atenolol': 134})\n",
      "Counter({1: 620, 4: 428, 0: 161, 3: 129, 5: 119, 2: 100})\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-bd0474de463d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0masklabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Drug'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresult_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweighted\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweighted_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mblood\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mblood_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mask_labels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprepareDatasetNoSmote\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0masklabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mres_acc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mres_scores\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSVM10FoldNoSmote\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresult_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Result\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mwei_acc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwei_scores\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSVM10FoldNoSmote\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweighted\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweighted_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Weighted\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mblood_acc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mblood_scores\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSVM10FoldNoSmote\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblood\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mblood_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Blood\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-79-f1675c6346ee>\u001b[0m in \u001b[0;36mSVM10FoldNoSmote\u001b[1;34m(result, result_labels, DataSet)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mresult_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "Stats = pd.DataFrame(columns = ['Dataset','Accuracy', 'Precision' , 'Recall','FScore'])\n",
    "\n",
    "label='Drug'\n",
    "asklabel='Drug'\n",
    "result,result_labels,weighted,weighted_labels,blood,blood_labels,ask,ask_labels=prepareDatasetNoSmote(label,asklabel)\n",
    "res_acc,res_scores=SVM10FoldNoSmote(result,result_labels,\"Result\")\n",
    "wei_acc,wei_scores=SVM10FoldNoSmote(weighted,weighted_labels,\"Weighted\")\n",
    "blood_acc,blood_scores=SVM10FoldNoSmote(blood,blood_labels,\"Blood\")\n",
    "ask_acc,ask_scores=SVM10FoldNoSmote(ask,ask_labels,\"Ask\")\n",
    "print(\"Res: Accuracy: \" + str(res_acc)+\" # \"+ \"Scores: \"+str(res_scores))\n",
    "print(\"Wei: Accuracy: \" + str(wei_acc)+\" # \"+ \"Scores: \"+str(wei_scores))\n",
    "print(\"Bld: Accuracy: \" + str(blood_acc)+\" # \"+ \"Scores: \"+str(blood_scores))\n",
    "print(\"Ask: Accuracy: \" + str(ask_acc)+\" # \"+ \"Scores: \"+str(ask_scores))\n",
    "Stats=Stats.append({'Dataset':\"Complete\",'Accuracy':res_acc,'Precision':res_scores[0],'Recall':res_scores[1],'FScore':res_scores[2]},ignore_index=True)\n",
    "Stats=Stats.append({'Dataset':\"Weighted\",'Accuracy':wei_acc,'Precision':wei_scores[0],'Recall':wei_scores[1],'FScore':wei_scores[2]},ignore_index=True)\n",
    "Stats=Stats.append({'Dataset':\"Blood\",'Accuracy':blood_acc,'Precision':blood_scores[0],'Recall':blood_scores[1],'FScore':blood_scores[2]},ignore_index=True)\n",
    "Stats=Stats.append({'Dataset':\"AskAPatient\",'Accuracy':ask_acc,'Precision':ask_scores[0],'Recall':ask_scores[1],'FScore':ask_scores[2]},ignore_index=True)\n",
    "Stats.to_excel('SVMresult_Stats10Fold'+label+'smoteless.xlsx') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Atenolol': 620, 'Lisinopril': 428, 'Amlodipine': 161, 'Hydrochlorothiazide': 129, 'Nadolol': 119, 'Diltiazem': 100})\n",
      "Counter({'Lisinopril': 51, 'Atenolol': 45, 'Amlodipine': 14, 'Hydrochlorothiazide': 13})\n",
      "Counter({'Atenolol': 186, 'Lisinopril': 134, 'Amlodipine': 63, 'Hydrochlorothiazide': 51, 'Diltiazem': 26})\n",
      "Counter({'Lisinopril': 622, 'Atenolol': 134})\n"
     ]
    }
   ],
   "source": [
    "result,result_labels,weighted,weighted_labels,blood,blood_labels,ask,ask_labels=prepareDatasetNoSmote(label,asklabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB(result,result_labels,DataSet):\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from collections import Counter\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(result, result_labels, test_size=0.30, random_state=42)\n",
    "    print(Counter(result_labels))\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_train, Y_train)\n",
    "    pred=clf.predict(X_test)\n",
    "    accuracy=(accuracy_score(Y_test, pred))\n",
    "    #print (accuracy)\n",
    "    scores_rf=(precision_recall_fscore_support(Y_test,pred, average='macro'))\n",
    "    #print(DataSet)\n",
    "    #print (pred)\n",
    "    #print(classification_report(Y_test,pred,labels=[0,1,2]))\n",
    "        \n",
    "    return accuracy,scores_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n",
      "Res: Accuracy: 0.5235042735042735 # Scores: (0.5223906705539358, 0.5175438596491229, 0.49288383325477775, None)\n",
      "Wei: Accuracy: 0.48717948717948717 # Scores: (0.45855614973262027, 0.4496753246753247, 0.44285714285714284, None)\n",
      "Bld: Accuracy: 0.6258992805755396 # Scores: (0.5817757009345794, 0.5618921308576481, 0.5564555719194895, None)\n",
      "Ask: Accuracy: 0.8370044052863436 # Scores: (0.4185022026431718, 0.5, 0.4556354916067146, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "label='Drug'\n",
    "asklabel='Drug'\n",
    "Stats = pd.DataFrame(columns = ['Dataset','Accuracy', 'Precision' , 'Recall','FScore'])\n",
    "\n",
    "\n",
    "result,result_labels,weighted,weighted_labels,blood,blood_labels,ask,ask_labels=prepareDataset(label,asklabel)\n",
    "res_acc,res_scores=NB(result,result_labels,\"Result\")\n",
    "wei_acc,wei_scores=NB(weighted,weighted_labels,\"Weighted\")\n",
    "blood_acc,blood_scores=NB(blood,blood_labels,\"Blood\")\n",
    "ask_acc,ask_scores=NB(ask,ask_labels,\"Ask\")\n",
    "print(\"Res: Accuracy: \" + str(res_acc)+\" # \"+ \"Scores: \"+str(res_scores))\n",
    "print(\"Wei: Accuracy: \" + str(wei_acc)+\" # \"+ \"Scores: \"+str(wei_scores))\n",
    "print(\"Bld: Accuracy: \" + str(blood_acc)+\" # \"+ \"Scores: \"+str(blood_scores))\n",
    "print(\"Ask: Accuracy: \" + str(ask_acc)+\" # \"+ \"Scores: \"+str(ask_scores))\n",
    "Stats=Stats.append({'Dataset':\"Complete\",'Accuracy':res_acc,'Precision':res_scores[0],'Recall':res_scores[1],'FScore':res_scores[2]},ignore_index=True)\n",
    "Stats=Stats.append({'Dataset':\"Weighted\",'Accuracy':wei_acc,'Precision':wei_scores[0],'Recall':wei_scores[1],'FScore':wei_scores[2]},ignore_index=True)\n",
    "Stats=Stats.append({'Dataset':\"Blood\",'Accuracy':blood_acc,'Precision':blood_scores[0],'Recall':blood_scores[1],'FScore':blood_scores[2]},ignore_index=True)\n",
    "Stats=Stats.append({'Dataset':\"AskAPatient\",'Accuracy':ask_acc,'Precision':ask_scores[0],'Recall':ask_scores[1],'FScore':ask_scores[2]},ignore_index=True)\n",
    "Stats.to_excel('NBresult_Stats'+label+'smoteless.xlsx') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label='Drug'\n",
    "asklabel='Drug'\n",
    "Stats = pd.DataFrame(columns = ['Dataset','Accuracy', 'Precision' , 'Recall','FScore'])\n",
    "\n",
    "\n",
    "result,result_labels,weighted,weighted_labels,blood,blood_labels,ask,ask_labels=prepareDataset(label,asklabel)\n",
    "res_acc,res_scores=NB(result,result_labels,\"Result\")\n",
    "wei_acc,wei_scores=NB(weighted,weighted_labels,\"Weighted\")\n",
    "blood_acc,blood_scores=NB(blood,blood_labels,\"Blood\")\n",
    "ask_acc,ask_scores=NB(ask,ask_labels,\"Ask\")\n",
    "print(\"Res: Accuracy: \" + str(res_acc)+\" # \"+ \"Scores: \"+str(res_scores))\n",
    "print(\"Wei: Accuracy: \" + str(wei_acc)+\" # \"+ \"Scores: \"+str(wei_scores))\n",
    "print(\"Bld: Accuracy: \" + str(blood_acc)+\" # \"+ \"Scores: \"+str(blood_scores))\n",
    "print(\"Ask: Accuracy: \" + str(ask_acc)+\" # \"+ \"Scores: \"+str(ask_scores))\n",
    "Stats=Stats.append({'Dataset':\"Complete\",'Accuracy':res_acc,'Precision':res_scores[0],'Recall':res_scores[1],'FScore':res_scores[2]},ignore_index=True)\n",
    "Stats=Stats.append({'Dataset':\"Weighted\",'Accuracy':wei_acc,'Precision':wei_scores[0],'Recall':wei_scores[1],'FScore':wei_scores[2]},ignore_index=True)\n",
    "Stats=Stats.append({'Dataset':\"Blood\",'Accuracy':blood_acc,'Precision':blood_scores[0],'Recall':blood_scores[1],'FScore':blood_scores[2]},ignore_index=True)\n",
    "Stats=Stats.append({'Dataset':\"AskAPatient\",'Accuracy':ask_acc,'Precision':ask_scores[0],'Recall':ask_scores[1],'FScore':ask_scores[2]},ignore_index=True)\n",
    "Stats.to_excel('NBresult_Stats'+label+'smoteless.xlsx') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes 10-Fold Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB10Fold(result,result_labels,DataSet):\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from collections import Counter\n",
    "    kf = KFold(n_splits=10)\n",
    "\n",
    "    accuracy=[]\n",
    "\n",
    "    scores_rf=[]\n",
    "    print(Counter(result_labels))\n",
    "    count = 1\n",
    "    for train_index, test_index in kf.split(result):\n",
    "        X_train, X_test, Y_train, Y_test = result[train_index], result[test_index], result_labels[train_index],result_labels[test_index]\n",
    "        clf = GaussianNB()\n",
    "        clf.fit(X_train, Y_train)\n",
    "        pred=clf.predict(X_test)\n",
    "        accuracy.append(accuracy_score(Y_test, pred))\n",
    "        scores_rf.append(precision_recall_fscore_support(Y_test,pred, average='macro'))\n",
    "        \n",
    "        #print(str(count)+DataSet)\n",
    "        #print(f1_score(Y_test, pred, average='micro'))\n",
    "        count+=1\n",
    "        #print (pred)\n",
    "        #print(classification_report(Y_test,pred,labels=[0,1]))\n",
    "    accuracy = np.mean(accuracy,axis=0)   \n",
    "    return accuracy,average(scores_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes 10-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stats = pd.DataFrame(columns = ['Dataset','Accuracy', 'Precision' , 'Recall','FScore'])\n",
    "\n",
    "label='Hypertensive disease '\n",
    "asklabel='Hypertensive disease'\n",
    "result,result_labels,weighted,weighted_labels,blood,blood_labels,ask,ask_labels=prepareDataset(label,asklabel)\n",
    "res_acc,res_scores=NB10Fold(result,result_labels,\"Result\")\n",
    "wei_acc,wei_scores=NB10Fold(weighted,weighted_labels,\"Weighted\")\n",
    "blood_acc,blood_scores=NB10Fold(blood,blood_labels,\"Blood\")\n",
    "ask_acc,ask_scores=NB10Fold(ask,ask_labels,\"Ask\")\n",
    "print(\"Res: Accuracy: \" + str(res_acc)+\" # \"+ \"Scores: \"+str(res_scores))\n",
    "print(\"Wei: Accuracy: \" + str(wei_acc)+\" # \"+ \"Scores: \"+str(wei_scores))\n",
    "print(\"Bld: Accuracy: \" + str(blood_acc)+\" # \"+ \"Scores: \"+str(blood_scores))\n",
    "print(\"Ask: Accuracy: \" + str(ask_acc)+\" # \"+ \"Scores: \"+str(ask_scores))\n",
    "Stats=Stats.append({'Dataset':\"Complete\",'Accuracy':res_acc,'Precision':res_scores[0],'Recall':res_scores[1],'FScore':res_scores[2]},ignore_index=True)\n",
    "Stats=Stats.append({'Dataset':\"Weighted\",'Accuracy':wei_acc,'Precision':wei_scores[0],'Recall':wei_scores[1],'FScore':wei_scores[2]},ignore_index=True)\n",
    "Stats=Stats.append({'Dataset':\"Blood\",'Accuracy':blood_acc,'Precision':blood_scores[0],'Recall':blood_scores[1],'FScore':blood_scores[2]},ignore_index=True)\n",
    "Stats=Stats.append({'Dataset':\"AskAPatient\",'Accuracy':ask_acc,'Precision':ask_scores[0],'Recall':ask_scores[1],'FScore':ask_scores[2]},ignore_index=True)\n",
    "Stats.to_excel('NBresult_Stats10Fold'+label+'.xlsx') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from collections import Counter\\nfrom imblearn.over_sampling import SMOTENC\\nsm = SMOTENC(random_state=42, categorical_features=[0, 1,3])\\nX_res, y_res = sm.fit_resample(result, result_labels)\\nprint(Counter(result_labels))\\nprint(Counter(y_res))\\nprint(X_res)\\nprint(y_res)\\nprint(result_labels)'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from collections import Counter\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "sm = SMOTENC(random_state=42, categorical_features=[0, 1,3])\n",
    "X_res, y_res = sm.fit_resample(result, result_labels)\n",
    "print(Counter(result_labels))\n",
    "print(Counter(y_res))\n",
    "print(X_res)\n",
    "print(y_res)\n",
    "print(result_labels)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-30204082ef7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwei_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-b1dfedaeb27b>\u001b[0m in \u001b[0;36maverage\u001b[1;34m(scores)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mPrecision\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mRecall\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mFScore\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "average(wei_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_labels.value_counts())\n",
    "print(weighted_labels.value_counts())\n",
    "print(blood_labels.value_counts())\n",
    "print(ask_labels.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_labels.value_counts())\n",
    "print(weighted_labels.value_counts())\n",
    "print(blood_labels.value_counts())\n",
    "print(ask_labels.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wei_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wei_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 287,
   "position": {
    "height": "309px",
    "left": "1303px",
    "right": "20px",
    "top": "39px",
    "width": "649px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
