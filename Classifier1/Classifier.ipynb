{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports necessary for running the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df = pd.read_excel(\"MLDataSet.xlsx\")\\ndf2 = pd.read_excel(\"ADRs.xlsx\")\\ndf3 = pd.read_excel(\"DS.xlsx\")\\ndf4 = pd.read_excel(\"Mental.xlsx\")\\n\\ndf5=[df, df2,  df3, df4]\\n\\nresult = pd.concat([df,df2,df3, df4], axis=1, join_axes=[df.index])'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun May  5 23:01:42 2019\n",
    "\n",
    "@author: Ahmed\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "'''df = pd.read_excel(\"MLDataSet.xlsx\")\n",
    "df2 = pd.read_excel(\"ADRs.xlsx\")\n",
    "df3 = pd.read_excel(\"DS.xlsx\")\n",
    "df4 = pd.read_excel(\"Mental.xlsx\")\n",
    "\n",
    "df5=[df, df2,  df3, df4]\n",
    "\n",
    "result = pd.concat([df,df2,df3, df4], axis=1, join_axes=[df.index])'''\n",
    "#result.to_excel('result.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeColumns(result):\n",
    "    del result['Pain']\n",
    "    del result['Content']\n",
    "    del result['Filtered']\n",
    "    del result['Stemmed']\n",
    "    del result['big1']\n",
    "    del result['big2']\n",
    "    del result['small1']\n",
    "    del result['small2']\n",
    "    del result['Height']\n",
    "    del result['Joined']\n",
    "    del result['Posted']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizeCount(result):\n",
    "    for i in range(len(result)):\n",
    "        if result[i]==0:\n",
    "            result[i]=0\n",
    "        if result[i]>0 and result[i]<=3:\n",
    "            result[i]=1\n",
    "        if result[i]>3:\n",
    "            result[i]=2\n",
    "    return result\n",
    "    '''if result.at[i,'MentalCount']==0:\n",
    "        result.at[i,'MentalCount']=0\n",
    "    if result.at[i,'MentalCount']>0 and result.at[i,'MentalCount']<=3:\n",
    "        result.at[i,'MentalCount']=1\n",
    "    if result.at[i,'MentalCount']>3:\n",
    "        result.at[i,'MentalCount']=2\n",
    "\n",
    "\n",
    "    if result.at[i,'DieaseCount']==0:\n",
    "        result.at[i,'DieaseCount']=0\n",
    "    if result.at[i,'DieaseCount']>0 and result.at[i,'DieaseCount']<=3:\n",
    "        result.at[i,'DieaseCount']=1\n",
    "    if result.at[i,'DieaseCount']>3:\n",
    "        result.at[i,'DieaseCount']=2'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manipulating data to prepare for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manipulate(result,labelfile,label):\n",
    "    imp_mean = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "    result['Gender']=imp_mean.fit_transform(result['Gender'].values.reshape(-1, 1))\n",
    "    for i in range(len(result)):\n",
    "        if result.at[i,'Gender']=='Male':\n",
    "            result.at[i,'Gender']=0\n",
    "        if result.at[i,'Gender']=='Female':\n",
    "            result.at[i,'Gender']=1\n",
    "    result['Drug']=LabelEncoder().fit_transform(result['Drug'])\n",
    "    if 'Unnamed: 0' in result:\n",
    "        del result['Unnamed: 0'] \n",
    "    result['DrugFamily']=LabelEncoder().fit_transform(result['DrugFamily'])\n",
    "    labels=labelfile.iloc[:][label]\n",
    "    \n",
    "    #removeColumns(result)\n",
    "    imp_mean = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "    result['Age']=imp_mean.fit_transform(result['Age'].values.reshape(-1, 1))\n",
    "    if 'Height' in result:\n",
    "        result['Height']=imp_mean.fit_transform(result['Height'].values.reshape(-1, 1))\n",
    "    return result,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#asklabel=pd.read_excel('asklabels.xlsx')\n",
    "#labels=asklabel.iloc[:]['MICROCEPHALY , EPILEPSY , AND DIABETES SYNDROME']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "label='ADRCount'\n",
    "asklabel='ADRCount'\n",
    "\n",
    "#from collections import Counter\n",
    "#X_res, y_res = sm.fit_resample(result, result_labels)\n",
    "#print(Counter(result_labels))\n",
    "#print(Counter(y_res))\n",
    "def prepareDataset(label,asklab):\n",
    "    from imblearn.over_sampling import SMOTENC\n",
    "    \n",
    "    sm = SMOTENC(random_state=42, categorical_features=[0, 1,3])\n",
    "    \n",
    "    result=pd.read_excel('result_reduced.xlsx')\n",
    "    weighted=pd.read_excel('Weighted_reduced.xlsx')\n",
    "    blood=pd.read_excel('blood_reduced.xlsx')\n",
    "    ask=pd.read_excel('askapatient_reduced.xlsx')\n",
    "\n",
    "    resultlabel=pd.read_excel('resultlabels.xlsx')\n",
    "    weightedlabel=pd.read_excel('Weightedlabels.xlsx')\n",
    "    bloodlabel=pd.read_excel('bloodlabels.xlsx')\n",
    "    asklabel=pd.read_excel('asklabels.xlsx')\n",
    "\n",
    "    result,result_labels=manipulate(result,resultlabel,label)\n",
    "    weighted,weighted_labels=manipulate(weighted,weightedlabel,label)\n",
    "    blood,blood_labels=manipulate(blood,bloodlabel,label)\n",
    "    ask,ask_labels=manipulate(ask,asklabel,asklab)\n",
    "    if 'Count' in label:\n",
    "        result_labels = vectorizeCount(result_labels)\n",
    "        weighted_labels = vectorizeCount(weighted_labels)\n",
    "        blood_labels=vectorizeCount(blood_labels)\n",
    "    if 'Count' in asklab:\n",
    "        ask_labels = vectorizeCount(ask_labels)\n",
    "    del result['weights2']\n",
    "    del result['Height']\n",
    "    \n",
    "    \n",
    "    #Applying SMOTENC\n",
    "    result,result_labels=sm.fit_resample(result,result_labels)\n",
    "    weighted,weighted_labels=sm.fit_resample(weighted,weighted_labels)\n",
    "    blood,blood_labels=sm.fit_resample(blood,blood_labels)\n",
    "    ask,ask_labels=sm.fit_resample(ask,ask_labels)\n",
    "    return result,result_labels,weighted,weighted_labels,blood,blood_labels,ask,ask_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "label='ADRCount'\n",
    "asklabel='ADRCount'\n",
    "\n",
    "#from collections import Counter\n",
    "#X_res, y_res = sm.fit_resample(result, result_labels)\n",
    "#print(Counter(result_labels))\n",
    "#print(Counter(y_res))\n",
    "def prepareDataset(label,asklab):\n",
    "    from imblearn.over_sampling import SMOTENC\n",
    "    \n",
    "    sm = SMOTENC(random_state=42, categorical_features=[0, 1,3])\n",
    "    \n",
    "    result=pd.read_excel('result_reduced.xlsx')\n",
    "    weighted=pd.read_excel('Weighted_reduced.xlsx')\n",
    "    blood=pd.read_excel('blood_reduced.xlsx')\n",
    "    ask=pd.read_excel('askapatient_reduced.xlsx')\n",
    "\n",
    "    resultlabel=pd.read_excel('resultlabels.xlsx')\n",
    "    weightedlabel=pd.read_excel('Weightedlabels.xlsx')\n",
    "    bloodlabel=pd.read_excel('bloodlabels.xlsx')\n",
    "    asklabel=pd.read_excel('asklabels.xlsx')\n",
    "\n",
    "    result,result_labels=manipulate(result,resultlabel,label)\n",
    "    weighted,weighted_labels=manipulate(weighted,weightedlabel,label)\n",
    "    blood,blood_labels=manipulate(blood,bloodlabel,label)\n",
    "    ask,ask_labels=manipulate(ask,asklabel,asklab)\n",
    "    if 'Count' in label:\n",
    "        result_labels = vectorizeCount(result_labels)\n",
    "        weighted_labels = vectorizeCount(weighted_labels)\n",
    "        blood_labels=vectorizeCount(blood_labels)\n",
    "    if 'Count' in asklab:\n",
    "        ask_labels = vectorizeCount(ask_labels)\n",
    "    del result['weights2']\n",
    "    del result['Height']\n",
    "    \n",
    "    \n",
    "    #Applying SMOTENC\n",
    "    #result,result_labels=sm.fit_resample(result,result_labels)\n",
    "    #weighted,weighted_labels=sm.fit_resample(weighted,weighted_labels)\n",
    "    #blood,blood_labels=sm.fit_resample(blood,blood_labels)\n",
    "    #ask,ask_labels=sm.fit_resample(ask,ask_labels)\n",
    "    return result,result_labels,weighted,weighted_labels,blood,blood_labels,ask,ask_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for i in weighted:\\n    if i not in result:\\n        if i != 'weights2':\\n            del weighted[i]\\nweighted.to_excel('Weighted.xlsx')\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for i in weighted:\n",
    "    if i not in result:\n",
    "        if i != 'weights2':\n",
    "            del weighted[i]\n",
    "weighted.to_excel('Weighted.xlsx')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ask_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for i in weighted:\\n    print (weighted[i])'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for i in weighted:\n",
    "    print (weighted[i])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(model, X_train,X_test,Y_train,Y_test):\n",
    "    model.fit(X_train,Y_train)\n",
    "    return model.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-Fold Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#cross_val_score(RandomForestClassifier(n_estimators=1000),result,result_labels,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This was used for some other no longer existing purposes\n",
    "def getMissingPercentage(feature):\n",
    "    #nancount = int(result[result[feature].isnull()][feature].shape[0])\n",
    "    nancount = int(result[result[feature]==1][feature].shape[0])\n",
    "    size=int(result.shape[0])\n",
    "    print (nancount)\n",
    "    print ((nancount*100)/size)\n",
    "#getMissingPercentage('Pvc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.model_selection import KFold\\nfrom sklearn.ensemble import RandomForestClassifier\\nkf = KFold(n_splits=10)\\n\\nscores_rf=[]\\n\\nfor train_index, test_index in kf.split(result):\\n    X_train, X_test, Y_train, Y_test = result.iloc[train_index], result.iloc[test_index], result_labels.iloc[train_index],result_labels.iloc[test_index]\\n    scores_rf.append(get_score(RandomForestClassifier(n_estimators=400),X_train, X_test, Y_train, Y_test))'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "scores_rf=[]\n",
    "\n",
    "for train_index, test_index in kf.split(result):\n",
    "    X_train, X_test, Y_train, Y_test = result.iloc[train_index], result.iloc[test_index], result_labels.iloc[train_index],result_labels.iloc[test_index]\n",
    "    scores_rf.append(get_score(RandomForestClassifier(n_estimators=400),X_train, X_test, Y_train, Y_test))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomForest(result,result_labels,estimators,DataSet):\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from collections import Counter\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(result, result_labels, test_size=0.30, random_state=42)\n",
    "    print(Counter(result_labels))\n",
    "    clf = RandomForestClassifier(n_estimators=estimators, max_depth=2,random_state=0)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    pred=clf.predict(X_test)\n",
    "    accuracy=(accuracy_score(Y_test, pred))\n",
    "    #print (accuracy)\n",
    "    scores_rf=(precision_recall_fscore_support(Y_test,pred, average='macro'))\n",
    "    #print(DataSet)\n",
    "    #print (pred)\n",
    "    #print(classification_report(Y_test,pred,labels=[0,1,2]))\n",
    "        \n",
    "    return accuracy,scores_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res: Accuracy: 0.5341880341880342 # Scores: (0.5423280423280423, 0.5263157894736842, 0.4808572998717714, None)\n",
      "Wei: Accuracy: 0.6410256410256411 # Scores: (0.5022321428571428, 0.5016233766233766, 0.4944444444444444, None)\n",
      "Bld: Accuracy: 0.6474820143884892 # Scores: (0.6145052473763118, 0.5675287356321839, 0.5526436781609196, None)\n",
      "Ask: Accuracy: 0.8370044052863436 # Scores: (0.4185022026431718, 0.5, 0.4556354916067146, None)\n",
      "200\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res: Accuracy: 0.532051282051282 # Scores: (0.5396415965245723, 0.5240131578947369, 0.47605244996549345, None)\n",
      "Wei: Accuracy: 0.6410256410256411 # Scores: (0.5022321428571428, 0.5016233766233766, 0.4944444444444444, None)\n",
      "Bld: Accuracy: 0.6402877697841727 # Scores: (0.6027310924369749, 0.5540450928381963, 0.5314185544768069, None)\n",
      "Ask: Accuracy: 0.8370044052863436 # Scores: (0.4185022026431718, 0.5, 0.4556354916067146, None)\n",
      "300\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res: Accuracy: 0.532051282051282 # Scores: (0.5396415965245723, 0.5240131578947369, 0.47605244996549345, None)\n",
      "Wei: Accuracy: 0.6923076923076923 # Scores: (0.5676470588235294, 0.5373376623376623, 0.528225806451613, None)\n",
      "Bld: Accuracy: 0.6330935251798561 # Scores: (0.5881759483454398, 0.5482979664014147, 0.5262946876044103, None)\n",
      "Ask: Accuracy: 0.8370044052863436 # Scores: (0.4185022026431718, 0.5, 0.4556354916067146, None)\n",
      "400\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res: Accuracy: 0.5363247863247863 # Scores: (0.5480675950555469, 0.5280701754385965, 0.4775254019292605, None)\n",
      "Wei: Accuracy: 0.6923076923076923 # Scores: (0.5676470588235294, 0.5373376623376623, 0.528225806451613, None)\n",
      "Bld: Accuracy: 0.6402877697841727 # Scores: (0.6027310924369749, 0.5540450928381963, 0.5314185544768069, None)\n",
      "Ask: Accuracy: 0.8370044052863436 # Scores: (0.4185022026431718, 0.5, 0.4556354916067146, None)\n",
      "500\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res: Accuracy: 0.5384615384615384 # Scores: (0.5521294073044357, 0.5301535087719298, 0.4790889415644646, None)\n",
      "Wei: Accuracy: 0.6666666666666666 # Scores: (0.5303030303030303, 0.5194805194805194, 0.5110896817743491, None)\n",
      "Bld: Accuracy: 0.6330935251798561 # Scores: (0.5881759483454398, 0.5482979664014147, 0.5262946876044103, None)\n",
      "Ask: Accuracy: 0.8370044052863436 # Scores: (0.4185022026431718, 0.5, 0.4556354916067146, None)\n",
      "600\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res: Accuracy: 0.5384615384615384 # Scores: (0.5521294073044357, 0.5301535087719298, 0.4790889415644646, None)\n",
      "Wei: Accuracy: 0.6666666666666666 # Scores: (0.5303030303030303, 0.5194805194805194, 0.5110896817743491, None)\n",
      "Bld: Accuracy: 0.6402877697841727 # Scores: (0.6027310924369749, 0.5540450928381963, 0.5314185544768069, None)\n",
      "Ask: Accuracy: 0.8370044052863436 # Scores: (0.4185022026431718, 0.5, 0.4556354916067146, None)\n",
      "700\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res: Accuracy: 0.5363247863247863 # Scores: (0.5488084984209015, 0.5279605263157895, 0.4758210619222372, None)\n",
      "Wei: Accuracy: 0.6666666666666666 # Scores: (0.5303030303030303, 0.5194805194805194, 0.5110896817743491, None)\n",
      "Bld: Accuracy: 0.6402877697841727 # Scores: (0.6027310924369749, 0.5540450928381963, 0.5314185544768069, None)\n",
      "Ask: Accuracy: 0.8370044052863436 # Scores: (0.4185022026431718, 0.5, 0.4556354916067146, None)\n",
      "800\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res: Accuracy: 0.5363247863247863 # Scores: (0.5488084984209015, 0.5279605263157895, 0.4758210619222372, None)\n",
      "Wei: Accuracy: 0.6666666666666666 # Scores: (0.5303030303030303, 0.5194805194805194, 0.5110896817743491, None)\n",
      "Bld: Accuracy: 0.6474820143884892 # Scores: (0.6186403508771929, 0.5597922192749779, 0.5365720895420834, None)\n",
      "Ask: Accuracy: 0.8370044052863436 # Scores: (0.4185022026431718, 0.5, 0.4556354916067146, None)\n",
      "900\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res: Accuracy: 0.5384615384615384 # Scores: (0.5521294073044357, 0.5301535087719298, 0.4790889415644646, None)\n",
      "Wei: Accuracy: 0.6666666666666666 # Scores: (0.5303030303030303, 0.5194805194805194, 0.5110896817743491, None)\n",
      "Bld: Accuracy: 0.6546762589928058 # Scores: (0.6361340679522498, 0.5655393457117595, 0.5417582417582417, None)\n",
      "Ask: Accuracy: 0.8370044052863436 # Scores: (0.4185022026431718, 0.5, 0.4556354916067146, None)\n",
      "1000\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res: Accuracy: 0.5363247863247863 # Scores: (0.5488084984209015, 0.5279605263157895, 0.4758210619222372, None)\n",
      "Wei: Accuracy: 0.6666666666666666 # Scores: (0.5303030303030303, 0.5194805194805194, 0.5110896817743491, None)\n",
      "Bld: Accuracy: 0.6546762589928058 # Scores: (0.6361340679522498, 0.5655393457117595, 0.5417582417582417, None)\n",
      "Ask: Accuracy: 0.8370044052863436 # Scores: (0.4185022026431718, 0.5, 0.4556354916067146, None)\n",
      "1100\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res: Accuracy: 0.5363247863247863 # Scores: (0.5488084984209015, 0.5279605263157895, 0.4758210619222372, None)\n",
      "Wei: Accuracy: 0.6666666666666666 # Scores: (0.5303030303030303, 0.5194805194805194, 0.5110896817743491, None)\n",
      "Bld: Accuracy: 0.6546762589928058 # Scores: (0.6361340679522498, 0.5655393457117595, 0.5417582417582417, None)\n",
      "Ask: Accuracy: 0.8370044052863436 # Scores: (0.4185022026431718, 0.5, 0.4556354916067146, None)\n",
      "1200\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res: Accuracy: 0.5363247863247863 # Scores: (0.5488084984209015, 0.5279605263157895, 0.4758210619222372, None)\n",
      "Wei: Accuracy: 0.6666666666666666 # Scores: (0.5303030303030303, 0.5194805194805194, 0.5110896817743491, None)\n",
      "Bld: Accuracy: 0.6618705035971223 # Scores: (0.65549662487946, 0.5712864721485411, 0.5469800984675126, None)\n",
      "Ask: Accuracy: 0.8370044052863436 # Scores: (0.4185022026431718, 0.5, 0.4556354916067146, None)\n",
      "1300\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res: Accuracy: 0.5384615384615384 # Scores: (0.5521294073044357, 0.5301535087719298, 0.4790889415644646, None)\n",
      "Wei: Accuracy: 0.6923076923076923 # Scores: (0.5676470588235294, 0.5373376623376623, 0.528225806451613, None)\n",
      "Bld: Accuracy: 0.6546762589928058 # Scores: (0.6361340679522498, 0.5655393457117595, 0.5417582417582417, None)\n",
      "Ask: Accuracy: 0.8370044052863436 # Scores: (0.4185022026431718, 0.5, 0.4556354916067146, None)\n",
      "1400\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res: Accuracy: 0.5384615384615384 # Scores: (0.5521294073044357, 0.5301535087719298, 0.4790889415644646, None)\n",
      "Wei: Accuracy: 0.6666666666666666 # Scores: (0.5303030303030303, 0.5194805194805194, 0.5110896817743491, None)\n",
      "Bld: Accuracy: 0.6618705035971223 # Scores: (0.65549662487946, 0.5712864721485411, 0.5469800984675126, None)\n",
      "Ask: Accuracy: 0.8370044052863436 # Scores: (0.4185022026431718, 0.5, 0.4556354916067146, None)\n",
      "1500\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res: Accuracy: 0.5384615384615384 # Scores: (0.5521294073044357, 0.5301535087719298, 0.4790889415644646, None)\n",
      "Wei: Accuracy: 0.6666666666666666 # Scores: (0.5303030303030303, 0.5194805194805194, 0.5110896817743491, None)\n",
      "Bld: Accuracy: 0.6546762589928058 # Scores: (0.6361340679522498, 0.5655393457117595, 0.5417582417582417, None)\n",
      "Ask: Accuracy: 0.8370044052863436 # Scores: (0.4185022026431718, 0.5, 0.4556354916067146, None)\n",
      "1600\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res: Accuracy: 0.5405982905982906 # Scores: (0.5562733275911571, 0.5322368421052631, 0.48065220420866817, None)\n",
      "Wei: Accuracy: 0.6666666666666666 # Scores: (0.5303030303030303, 0.5194805194805194, 0.5110896817743491, None)\n",
      "Bld: Accuracy: 0.6618705035971223 # Scores: (0.65549662487946, 0.5712864721485411, 0.5469800984675126, None)\n",
      "Ask: Accuracy: 0.8370044052863436 # Scores: (0.4185022026431718, 0.5, 0.4556354916067146, None)\n",
      "1700\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res: Accuracy: 0.5384615384615384 # Scores: (0.5521294073044357, 0.5301535087719298, 0.4790889415644646, None)\n",
      "Wei: Accuracy: 0.6666666666666666 # Scores: (0.5303030303030303, 0.5194805194805194, 0.5110896817743491, None)\n",
      "Bld: Accuracy: 0.6618705035971223 # Scores: (0.65549662487946, 0.5712864721485411, 0.5469800984675126, None)\n",
      "Ask: Accuracy: 0.8370044052863436 # Scores: (0.4185022026431718, 0.5, 0.4556354916067146, None)\n",
      "1800\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res: Accuracy: 0.5384615384615384 # Scores: (0.5521294073044357, 0.5301535087719298, 0.4790889415644646, None)\n",
      "Wei: Accuracy: 0.6666666666666666 # Scores: (0.5303030303030303, 0.5194805194805194, 0.5110896817743491, None)\n",
      "Bld: Accuracy: 0.6618705035971223 # Scores: (0.65549662487946, 0.5712864721485411, 0.5469800984675126, None)\n",
      "Ask: Accuracy: 0.8370044052863436 # Scores: (0.4185022026431718, 0.5, 0.4556354916067146, None)\n",
      "1900\n",
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res: Accuracy: 0.5384615384615384 # Scores: (0.5521294073044357, 0.5301535087719298, 0.4790889415644646, None)\n",
      "Wei: Accuracy: 0.6666666666666666 # Scores: (0.5303030303030303, 0.5194805194805194, 0.5110896817743491, None)\n",
      "Bld: Accuracy: 0.6618705035971223 # Scores: (0.65549662487946, 0.5712864721485411, 0.5469800984675126, None)\n",
      "Ask: Accuracy: 0.8370044052863436 # Scores: (0.4185022026431718, 0.5, 0.4556354916067146, None)\n"
     ]
    }
   ],
   "source": [
    "label='Hypertensive disease '\n",
    "asklabel='Hypertensive disease '\n",
    "res_Stats = pd.DataFrame(columns = ['Estimators' , 'Accuracy', 'Precision' , 'Recall','FScore'])\n",
    "wei_Stats = pd.DataFrame(columns = ['Estimators' , 'Accuracy', 'Precision' , 'Recall','FScore'])\n",
    "bld_Stats = pd.DataFrame(columns = ['Estimators' , 'Accuracy', 'Precision' , 'Recall','FScore'])\n",
    "ask_Stats = pd.DataFrame(columns = ['Estimators' , 'Accuracy', 'Precision' , 'Recall','FScore'])\n",
    "for estimators in range (100,2000,100):\n",
    "    print (estimators)\n",
    "    result,result_labels,weighted,weighted_labels,blood,blood_labels,ask,ask_labels=prepareDataset(label,asklabel)\n",
    "    res_acc,res_scores=randomForest(result,result_labels,estimators,\"Result\")\n",
    "    wei_acc,wei_scores=randomForest(weighted,weighted_labels,estimators,\"Weighted\")\n",
    "    blood_acc,blood_scores=randomForest(blood,blood_labels,estimators,\"Blood\")\n",
    "    ask_acc,ask_scores=randomForest(ask,ask_labels,estimators,\"Ask\")\n",
    "    print(\"Res: Accuracy: \" + str(res_acc)+\" # \"+ \"Scores: \"+str(res_scores))\n",
    "    print(\"Wei: Accuracy: \" + str(wei_acc)+\" # \"+ \"Scores: \"+str(wei_scores))\n",
    "    print(\"Bld: Accuracy: \" + str(blood_acc)+\" # \"+ \"Scores: \"+str(blood_scores))\n",
    "    print(\"Ask: Accuracy: \" + str(ask_acc)+\" # \"+ \"Scores: \"+str(ask_scores))\n",
    "    res_Stats=res_Stats.append({'Estimators':estimators,'Accuracy':res_acc,'Precision':res_scores[0],'Recall':res_scores[1],'FScore':res_scores[2]},ignore_index=True)\n",
    "    wei_Stats=wei_Stats.append({'Estimators':estimators,'Accuracy':wei_acc,'Precision':wei_scores[0],'Recall':wei_scores[1],'FScore':wei_scores[2]},ignore_index=True)\n",
    "    bld_Stats=bld_Stats.append({'Estimators':estimators,'Accuracy':blood_acc,'Precision':blood_scores[0],'Recall':blood_scores[1],'FScore':blood_scores[2]},ignore_index=True)\n",
    "    ask_Stats=ask_Stats.append({'Estimators':estimators,'Accuracy':ask_acc,'Precision':ask_scores[0],'Recall':ask_scores[1],'FScore':ask_scores[2]},ignore_index=True)\n",
    "res_Stats.to_excel('Forrestresult_Stats'+label+'smoteless.xlsx') \n",
    "wei_Stats.to_excel('Forrestweight_Stats'+label+'smoteless.xlsx') \n",
    "bld_Stats.to_excel('Forrestblood_Stats'+label+'smoteless.xlsx') \n",
    "ask_Stats.to_excel('Forrestask_Stats'+asklabel+'smoteless.xlsx') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier Function 10-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average(scores):\n",
    "    FScore=0\n",
    "    Recall=0\n",
    "    Precision=0\n",
    "\n",
    "    for i in scores:\n",
    "        Precision+=i[0]\n",
    "        Recall+=i[1]\n",
    "        FScore+=i[2]\n",
    "    return (Precision/10,Recall/10,FScore/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomForest10Fold(result,result_labels,estimators,DataSet):\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from collections import Counter\n",
    "    kf = KFold(n_splits=10)\n",
    "\n",
    "    accuracy=[]\n",
    "\n",
    "    scores_rf=[]\n",
    "    print(Counter(result_labels))\n",
    "    count = 1\n",
    "    for train_index, test_index in kf.split(result):\n",
    "        X_train, X_test, Y_train, Y_test = result[train_index], result[test_index], result_labels[train_index],result_labels[test_index]\n",
    "        clf = RandomForestClassifier(n_estimators=estimators, max_depth=2,random_state=0)\n",
    "        clf.fit(X_train, Y_train)\n",
    "        pred=clf.predict(X_test)\n",
    "        accuracy.append(accuracy_score(Y_test, pred))\n",
    "        scores_rf.append(precision_recall_fscore_support(Y_test,pred, average='micro'))\n",
    "        \n",
    "        #print(str(count)+DataSet)\n",
    "        #print(f1_score(Y_test, pred, average='micro'))\n",
    "        count+=1\n",
    "        #print (pred)\n",
    "        #print(classification_report(Y_test,pred,labels=[0,1]))\n",
    "    accuracy = np.mean(accuracy,axis=0)   \n",
    "    return accuracy,average(scores_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest with 10-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.2137214758925285 # Scores: (0.2137214758925285, 0.2137214758925285, 0.2137214758925285)\n",
      "Wei: Accuracy: 0.2757142857142857 # Scores: (0.2757142857142857, 0.2757142857142857, 0.2757142857142857)\n",
      "Bld: Accuracy: 0.29458653026427967 # Scores: (0.2945865302642796, 0.2945865302642796, 0.2945865302642796)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.20939915347810087 # Scores: (0.20939915347810084, 0.20939915347810084, 0.20939915347810084)\n",
      "Wei: Accuracy: 0.30476190476190473 # Scores: (0.3047619047619048, 0.3047619047619048, 0.3047619047619048)\n",
      "Bld: Accuracy: 0.30328218243819266 # Scores: (0.3032821824381927, 0.3032821824381927, 0.3032821824381927)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.21083456017666546 # Scores: (0.21083456017666546, 0.21083456017666546, 0.21083456017666546)\n",
      "Wei: Accuracy: 0.2914285714285714 # Scores: (0.2914285714285715, 0.2914285714285715, 0.2914285714285715)\n",
      "Bld: Accuracy: 0.3018329070758738 # Scores: (0.30183290707587385, 0.30183290707587385, 0.30183290707587385)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.21083456017666546 # Scores: (0.21083456017666546, 0.21083456017666546, 0.21083456017666546)\n",
      "Wei: Accuracy: 0.2780952380952381 # Scores: (0.2780952380952381, 0.2780952380952381, 0.2780952380952381)\n",
      "Bld: Accuracy: 0.2959931798806479 # Scores: (0.2959931798806479, 0.2959931798806479, 0.2959931798806479)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.21083456017666546 # Scores: (0.21083456017666546, 0.21083456017666546, 0.21083456017666546)\n",
      "Wei: Accuracy: 0.2857142857142857 # Scores: (0.2857142857142857, 0.2857142857142857, 0.2857142857142857)\n",
      "Bld: Accuracy: 0.2930946291560102 # Scores: (0.2930946291560102, 0.2930946291560102, 0.2930946291560102)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.21083456017666546 # Scores: (0.21083456017666546, 0.21083456017666546, 0.21083456017666546)\n",
      "Wei: Accuracy: 0.28523809523809524 # Scores: (0.28523809523809524, 0.28523809523809524, 0.28523809523809524)\n",
      "Bld: Accuracy: 0.2959931798806479 # Scores: (0.2959931798806479, 0.2959931798806479, 0.2959931798806479)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.20939685314685316 # Scores: (0.20939685314685313, 0.20939685314685313, 0.20939685314685313)\n",
      "Wei: Accuracy: 0.2985714285714286 # Scores: (0.2985714285714286, 0.2985714285714286, 0.2985714285714286)\n",
      "Bld: Accuracy: 0.2945865302642796 # Scores: (0.2945865302642796, 0.2945865302642796, 0.2945865302642796)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.2098753220463747 # Scores: (0.20987532204637466, 0.20987532204637466, 0.20987532204637466)\n",
      "Wei: Accuracy: 0.2842857142857143 # Scores: (0.2842857142857143, 0.2842857142857143, 0.2842857142857143)\n",
      "Bld: Accuracy: 0.2930946291560102 # Scores: (0.2930946291560102, 0.2930946291560102, 0.2930946291560102)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.2098753220463747 # Scores: (0.20987532204637466, 0.20987532204637466, 0.20987532204637466)\n",
      "Wei: Accuracy: 0.28476190476190477 # Scores: (0.28476190476190477, 0.28476190476190477, 0.28476190476190477)\n",
      "Bld: Accuracy: 0.29744245524296675 # Scores: (0.29744245524296675, 0.29744245524296675, 0.29744245524296675)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.21035609127714391 # Scores: (0.21035609127714391, 0.21035609127714391, 0.21035609127714391)\n",
      "Wei: Accuracy: 0.2771428571428572 # Scores: (0.2771428571428572, 0.2771428571428572, 0.2771428571428572)\n",
      "Bld: Accuracy: 0.29744245524296675 # Scores: (0.29744245524296675, 0.29744245524296675, 0.29744245524296675)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.20892068457857932 # Scores: (0.20892068457857932, 0.20892068457857932, 0.20892068457857932)\n",
      "Wei: Accuracy: 0.28380952380952384 # Scores: (0.28380952380952384, 0.28380952380952384, 0.28380952380952384)\n",
      "Bld: Accuracy: 0.2989343563512361 # Scores: (0.29893435635123616, 0.29893435635123616, 0.29893435635123616)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.2108345601766654 # Scores: (0.21083456017666546, 0.21083456017666546, 0.21083456017666546)\n",
      "Wei: Accuracy: 0.2771428571428572 # Scores: (0.2771428571428572, 0.2771428571428572, 0.2771428571428572)\n",
      "Bld: Accuracy: 0.300383631713555 # Scores: (0.300383631713555, 0.300383631713555, 0.300383631713555)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.21035609127714391 # Scores: (0.21035609127714391, 0.21035609127714391, 0.21035609127714391)\n",
      "Wei: Accuracy: 0.29095238095238096 # Scores: (0.29095238095238096, 0.29095238095238096, 0.29095238095238096)\n",
      "Bld: Accuracy: 0.297463768115942 # Scores: (0.297463768115942, 0.297463768115942, 0.297463768115942)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.21035609127714391 # Scores: (0.21035609127714391, 0.21035609127714391, 0.21035609127714391)\n",
      "Wei: Accuracy: 0.29809523809523814 # Scores: (0.2980952380952381, 0.2980952380952381, 0.29809523809523814)\n",
      "Bld: Accuracy: 0.2989343563512361 # Scores: (0.2989343563512361, 0.2989343563512361, 0.2989343563512361)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.21035609127714391 # Scores: (0.21035609127714391, 0.21035609127714391, 0.21035609127714391)\n",
      "Wei: Accuracy: 0.29809523809523814 # Scores: (0.2980952380952381, 0.2980952380952381, 0.29809523809523814)\n",
      "Bld: Accuracy: 0.300383631713555 # Scores: (0.300383631713555, 0.300383631713555, 0.300383631713555)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.2108345601766654 # Scores: (0.21083456017666546, 0.21083456017666546, 0.21083456017666546)\n",
      "Wei: Accuracy: 0.2914285714285715 # Scores: (0.2914285714285715, 0.2914285714285715, 0.2914285714285715)\n",
      "Bld: Accuracy: 0.3018329070758738 # Scores: (0.3018329070758738, 0.3018329070758738, 0.3018329070758738)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.2108345601766654 # Scores: (0.21083456017666546, 0.21083456017666546, 0.21083456017666546)\n",
      "Wei: Accuracy: 0.2842857142857143 # Scores: (0.28428571428571436, 0.28428571428571436, 0.28428571428571436)\n",
      "Bld: Accuracy: 0.29889173060528557 # Scores: (0.29889173060528557, 0.29889173060528557, 0.29889173060528557)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.21035609127714391 # Scores: (0.21035609127714391, 0.21035609127714391, 0.21035609127714391)\n",
      "Wei: Accuracy: 0.29095238095238096 # Scores: (0.29095238095238096, 0.29095238095238096, 0.29095238095238096)\n",
      "Bld: Accuracy: 0.29889173060528557 # Scores: (0.29889173060528557, 0.29889173060528557, 0.29889173060528557)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n",
      "Counter({0: 695, 1: 695, 2: 695})\n",
      "Counter({0: 48, 1: 48, 2: 48})\n",
      "Counter({1: 229, 0: 229, 2: 229})\n",
      "Counter({0: 493, 2: 493, 1: 493})\n",
      "Res: Accuracy: 0.2084422156790578 # Scores: (0.2084422156790578, 0.2084422156790578, 0.2084422156790578)\n",
      "Wei: Accuracy: 0.29095238095238096 # Scores: (0.29095238095238096, 0.29095238095238096, 0.29095238095238096)\n",
      "Bld: Accuracy: 0.29744245524296675 # Scores: (0.29744245524296675, 0.29744245524296675, 0.29744245524296675)\n",
      "Ask: Accuracy: 0.45245909174480603 # Scores: (0.45245909174480603, 0.45245909174480603, 0.45245909174480603)\n"
     ]
    }
   ],
   "source": [
    "label='ADRCount'\n",
    "asklabel='ADRCount'\n",
    "\n",
    "res_Stats = pd.DataFrame(columns = ['Estimators' , 'Accuracy', 'Precision' , 'Recall','FScore'])\n",
    "wei_Stats = pd.DataFrame(columns = ['Estimators' , 'Accuracy', 'Precision' , 'Recall','FScore'])\n",
    "bld_Stats = pd.DataFrame(columns = ['Estimators' , 'Accuracy', 'Precision' , 'Recall','FScore'])\n",
    "ask_Stats = pd.DataFrame(columns = ['Estimators' , 'Accuracy', 'Precision' , 'Recall','FScore'])\n",
    "for estimators in range (100,2000,100):\n",
    "    result,result_labels,weighted,weighted_labels,blood,blood_labels,ask,ask_labels=prepareDataset(label,asklabel)\n",
    "    res_acc,res_scores=randomForest10Fold(result,result_labels,estimators,\"Result\")\n",
    "    wei_acc,wei_scores=randomForest10Fold(weighted,weighted_labels,estimators,\"Weighted\")\n",
    "    blood_acc,blood_scores=randomForest10Fold(blood,blood_labels,estimators,\"Blood\")\n",
    "    ask_acc,ask_scores=randomForest10Fold(ask,ask_labels,1000,\"Ask\")\n",
    "    print(\"Res: Accuracy: \" + str(res_acc)+\" # \"+ \"Scores: \"+str(res_scores))\n",
    "    print(\"Wei: Accuracy: \" + str(wei_acc)+\" # \"+ \"Scores: \"+str(wei_scores))\n",
    "    print(\"Bld: Accuracy: \" + str(blood_acc)+\" # \"+ \"Scores: \"+str(blood_scores))\n",
    "    print(\"Ask: Accuracy: \" + str(ask_acc)+\" # \"+ \"Scores: \"+str(ask_scores))\n",
    "    res_Stats=res_Stats.append({'Estimators':estimators,'Accuracy':res_acc,'Precision':res_scores[0],'Recall':res_scores[1],'FScore':res_scores[2]},ignore_index=True)\n",
    "    wei_Stats=wei_Stats.append({'Estimators':estimators,'Accuracy':wei_acc,'Precision':wei_scores[0],'Recall':wei_scores[1],'FScore':wei_scores[2]},ignore_index=True)\n",
    "    bld_Stats=bld_Stats.append({'Estimators':estimators,'Accuracy':blood_acc,'Precision':blood_scores[0],'Recall':blood_scores[1],'FScore':blood_scores[2]},ignore_index=True)\n",
    "    ask_Stats=ask_Stats.append({'Estimators':estimators,'Accuracy':ask_acc,'Precision':ask_scores[0],'Recall':ask_scores[1],'FScore':ask_scores[2]},ignore_index=True)\n",
    "res_Stats.to_excel('ForrestFoldresult_Stats'+label+'.xlsx') \n",
    "wei_Stats.to_excel('ForrestFoldweight_Stats'+label+'.xlsx') \n",
    "bld_Stats.to_excel('ForrestFoldblood_Stats'+label+'.xlsx') \n",
    "ask_Stats.to_excel('ForrestFoldask_Stats'+asklabel+'.xlsx') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ask_scores[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Classifier Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(result,result_labels,DataSet):\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from collections import Counter\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(result, result_labels, test_size=0.30, random_state=42)\n",
    "    print(Counter(result_labels))\n",
    "    clf = SVC(gamma='auto')\n",
    "    clf.fit(X_train, Y_train)\n",
    "    pred=clf.predict(X_test)\n",
    "    accuracy=(accuracy_score(Y_test, pred))\n",
    "    #print (accuracy)\n",
    "    scores_rf=(precision_recall_fscore_support(Y_test,pred, average='macro'))\n",
    "    #print(DataSet)\n",
    "    #print (pred)\n",
    "    #print(classification_report(Y_test,pred,labels=[0,1,2]))\n",
    "        \n",
    "    return accuracy,scores_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n",
      "Res: Accuracy: 0.5918803418803419 # Scores: (0.5993464052287582, 0.5875, 0.5772717599489252, None)\n",
      "Wei: Accuracy: 0.7692307692307693 # Scores: (0.8783783783783784, 0.5909090909090909, 0.5846153846153846, None)\n",
      "Bld: Accuracy: 0.7050359712230215 # Scores: (0.728410008071025, 0.6251105216622458, 0.6191780821917808, None)\n",
      "Ask: Accuracy: 0.8370044052863436 # Scores: (0.4185022026431718, 0.5, 0.4556354916067146, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"res_Stats.to_excel('Forrestresult_Stats'+label+'smoteless.xlsx') \\nwei_Stats.to_excel('Forrestweight_Stats'+label+'smoteless.xlsx') \\nbld_Stats.to_excel('Forrestblood_Stats'+label+'smoteless.xlsx') \\nask_Stats.to_excel('Forrestask_Stats'+asklabel+'smoteless.xlsx') \""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label='Hypertensive disease '\n",
    "asklabel='Hypertensive disease '\n",
    "Stats = pd.DataFrame(columns = ['Dataset','Accuracy', 'Precision' , 'Recall','FScore'])\n",
    "\n",
    "\n",
    "result,result_labels,weighted,weighted_labels,blood,blood_labels,ask,ask_labels=prepareDataset(label,asklabel)\n",
    "res_acc,res_scores=SVM(result,result_labels,\"Result\")\n",
    "wei_acc,wei_scores=SVM(weighted,weighted_labels,\"Weighted\")\n",
    "blood_acc,blood_scores=SVM(blood,blood_labels,\"Blood\")\n",
    "ask_acc,ask_scores=SVM(ask,ask_labels,\"Ask\")\n",
    "print(\"Res: Accuracy: \" + str(res_acc)+\" # \"+ \"Scores: \"+str(res_scores))\n",
    "print(\"Wei: Accuracy: \" + str(wei_acc)+\" # \"+ \"Scores: \"+str(wei_scores))\n",
    "print(\"Bld: Accuracy: \" + str(blood_acc)+\" # \"+ \"Scores: \"+str(blood_scores))\n",
    "print(\"Ask: Accuracy: \" + str(ask_acc)+\" # \"+ \"Scores: \"+str(ask_scores))\n",
    "Stats=Stats.append({'Dataset':\"Complete\",'Accuracy':res_acc,'Precision':res_scores[0],'Recall':res_scores[1],'FScore':res_scores[2]},ignore_index=True)\n",
    "Stats=Stats.append({'Dataset':\"Weighted\",'Accuracy':wei_acc,'Precision':wei_scores[0],'Recall':wei_scores[1],'FScore':wei_scores[2]},ignore_index=True)\n",
    "Stats=Stats.append({'Dataset':\"Blood\",'Accuracy':blood_acc,'Precision':blood_scores[0],'Recall':blood_scores[1],'FScore':blood_scores[2]},ignore_index=True)\n",
    "Stats=Stats.append({'Dataset':\"AskAPatient\",'Accuracy':ask_acc,'Precision':ask_scores[0],'Recall':ask_scores[1],'FScore':ask_scores[2]},ignore_index=True)\n",
    "Stats.to_excel('SVMresult_Stats'+label+'.xlsx') \n",
    "'''res_Stats.to_excel('Forrestresult_Stats'+label+'smoteless.xlsx') \n",
    "wei_Stats.to_excel('Forrestweight_Stats'+label+'smoteless.xlsx') \n",
    "bld_Stats.to_excel('Forrestblood_Stats'+label+'smoteless.xlsx') \n",
    "ask_Stats.to_excel('Forrestask_Stats'+asklabel+'smoteless.xlsx') '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM 10-Fold Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM10Fold(result,result_labels,DataSet):\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from collections import Counter\n",
    "    kf = KFold(n_splits=10)\n",
    "\n",
    "    accuracy=[]\n",
    "\n",
    "    scores_rf=[]\n",
    "    print(Counter(result_labels))\n",
    "    count = 1\n",
    "    for train_index, test_index in kf.split(result):\n",
    "        X_train, X_test, Y_train, Y_test = result[train_index], result[test_index], result_labels[train_index],result_labels[test_index]\n",
    "        clf = SVC(gamma='auto')\n",
    "        clf.fit(X_train, Y_train)\n",
    "        pred=clf.predict(X_test)\n",
    "        accuracy.append(accuracy_score(Y_test, pred))\n",
    "        scores_rf.append(precision_recall_fscore_support(Y_test,pred, average='micro'))\n",
    "        \n",
    "        #print(str(count)+DataSet)\n",
    "        #print(f1_score(Y_test, pred, average='micro'))\n",
    "        count+=1\n",
    "        #print (pred)\n",
    "        #print(classification_report(Y_test,pred,labels=[0,1]))\n",
    "    accuracy = np.mean(accuracy,axis=0)   \n",
    "    return accuracy,average(scores_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM 10-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n",
      "Res: Accuracy: 0.5452646815550042 # Scores: (0.5452646815550042, 0.5452646815550042, 0.5452646815550042)\n",
      "Wei: Accuracy: 0.6846153846153846 # Scores: (0.6846153846153845, 0.6846153846153845, 0.6846153846153845)\n",
      "Bld: Accuracy: 0.6685476410730804 # Scores: (0.6685476410730804, 0.6685476410730804, 0.6685476410730804)\n",
      "Ask: Accuracy: 0.8039298245614035 # Scores: (0.8039298245614035, 0.8039298245614035, 0.8039298245614035)\n"
     ]
    }
   ],
   "source": [
    "Stats = pd.DataFrame(columns = ['Dataset','Accuracy', 'Precision' , 'Recall','FScore'])\n",
    "\n",
    "label='Hypertensive disease '\n",
    "asklabel='Hypertensive disease'\n",
    "result,result_labels,weighted,weighted_labels,blood,blood_labels,ask,ask_labels=prepareDataset(label,asklabel)\n",
    "res_acc,res_scores=SVM10Fold(result,result_labels,\"Result\")\n",
    "wei_acc,wei_scores=SVM10Fold(weighted,weighted_labels,\"Weighted\")\n",
    "blood_acc,blood_scores=SVM10Fold(blood,blood_labels,\"Blood\")\n",
    "ask_acc,ask_scores=SVM10Fold(ask,ask_labels,\"Ask\")\n",
    "print(\"Res: Accuracy: \" + str(res_acc)+\" # \"+ \"Scores: \"+str(res_scores))\n",
    "print(\"Wei: Accuracy: \" + str(wei_acc)+\" # \"+ \"Scores: \"+str(wei_scores))\n",
    "print(\"Bld: Accuracy: \" + str(blood_acc)+\" # \"+ \"Scores: \"+str(blood_scores))\n",
    "print(\"Ask: Accuracy: \" + str(ask_acc)+\" # \"+ \"Scores: \"+str(ask_scores))\n",
    "Stats=Stats.append({'Dataset':\"Complete\",'Accuracy':res_acc,'Precision':res_scores[0],'Recall':res_scores[1],'FScore':res_scores[2]},ignore_index=True)\n",
    "Stats=Stats.append({'Dataset':\"Weighted\",'Accuracy':wei_acc,'Precision':wei_scores[0],'Recall':wei_scores[1],'FScore':wei_scores[2]},ignore_index=True)\n",
    "Stats=Stats.append({'Dataset':\"Blood\",'Accuracy':blood_acc,'Precision':blood_scores[0],'Recall':blood_scores[1],'FScore':blood_scores[2]},ignore_index=True)\n",
    "Stats=Stats.append({'Dataset':\"AskAPatient\",'Accuracy':ask_acc,'Precision':ask_scores[0],'Recall':ask_scores[1],'FScore':ask_scores[2]},ignore_index=True)\n",
    "Stats.to_excel('SVMresult_Stats10Fold'+label+'.xlsx') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB(result,result_labels,DataSet):\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from collections import Counter\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(result, result_labels, test_size=0.30, random_state=42)\n",
    "    print(Counter(result_labels))\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_train, Y_train)\n",
    "    pred=clf.predict(X_test)\n",
    "    accuracy=(accuracy_score(Y_test, pred))\n",
    "    #print (accuracy)\n",
    "    scores_rf=(precision_recall_fscore_support(Y_test,pred, average='macro'))\n",
    "    #print(DataSet)\n",
    "    #print (pred)\n",
    "    #print(classification_report(Y_test,pred,labels=[0,1,2]))\n",
    "        \n",
    "    return accuracy,scores_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 834, 1: 723})\n",
      "Counter({1: 80, 0: 50})\n",
      "Counter({1: 274, 0: 188})\n",
      "Counter({0: 608, 1: 148})\n",
      "Res: Accuracy: 0.5235042735042735 # Scores: (0.5223906705539358, 0.5175438596491229, 0.49288383325477775, None)\n",
      "Wei: Accuracy: 0.48717948717948717 # Scores: (0.45855614973262027, 0.4496753246753247, 0.44285714285714284, None)\n",
      "Bld: Accuracy: 0.6258992805755396 # Scores: (0.5817757009345794, 0.5618921308576481, 0.5564555719194895, None)\n",
      "Ask: Accuracy: 0.8370044052863436 # Scores: (0.4185022026431718, 0.5, 0.4556354916067146, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "label='Hypertensive disease '\n",
    "asklabel='Hypertensive disease '\n",
    "Stats = pd.DataFrame(columns = ['Dataset','Accuracy', 'Precision' , 'Recall','FScore'])\n",
    "\n",
    "\n",
    "result,result_labels,weighted,weighted_labels,blood,blood_labels,ask,ask_labels=prepareDataset(label,asklabel)\n",
    "res_acc,res_scores=NB(result,result_labels,\"Result\")\n",
    "wei_acc,wei_scores=NB(weighted,weighted_labels,\"Weighted\")\n",
    "blood_acc,blood_scores=NB(blood,blood_labels,\"Blood\")\n",
    "ask_acc,ask_scores=NB(ask,ask_labels,\"Ask\")\n",
    "print(\"Res: Accuracy: \" + str(res_acc)+\" # \"+ \"Scores: \"+str(res_scores))\n",
    "print(\"Wei: Accuracy: \" + str(wei_acc)+\" # \"+ \"Scores: \"+str(wei_scores))\n",
    "print(\"Bld: Accuracy: \" + str(blood_acc)+\" # \"+ \"Scores: \"+str(blood_scores))\n",
    "print(\"Ask: Accuracy: \" + str(ask_acc)+\" # \"+ \"Scores: \"+str(ask_scores))\n",
    "Stats=Stats.append({'Dataset':\"Complete\",'Accuracy':res_acc,'Precision':res_scores[0],'Recall':res_scores[1],'FScore':res_scores[2]},ignore_index=True)\n",
    "Stats=Stats.append({'Dataset':\"Weighted\",'Accuracy':wei_acc,'Precision':wei_scores[0],'Recall':wei_scores[1],'FScore':wei_scores[2]},ignore_index=True)\n",
    "Stats=Stats.append({'Dataset':\"Blood\",'Accuracy':blood_acc,'Precision':blood_scores[0],'Recall':blood_scores[1],'FScore':blood_scores[2]},ignore_index=True)\n",
    "Stats=Stats.append({'Dataset':\"AskAPatient\",'Accuracy':ask_acc,'Precision':ask_scores[0],'Recall':ask_scores[1],'FScore':ask_scores[2]},ignore_index=True)\n",
    "Stats.to_excel('NBresult_Stats'+label+'smoteless.xlsx') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes 10-Fold Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB10Fold(result,result_labels,DataSet):\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import f1_score\n",
    "    from collections import Counter\n",
    "    kf = KFold(n_splits=10)\n",
    "\n",
    "    accuracy=[]\n",
    "\n",
    "    scores_rf=[]\n",
    "    print(Counter(result_labels))\n",
    "    count = 1\n",
    "    for train_index, test_index in kf.split(result):\n",
    "        X_train, X_test, Y_train, Y_test = result[train_index], result[test_index], result_labels[train_index],result_labels[test_index]\n",
    "        clf = GaussianNB()\n",
    "        clf.fit(X_train, Y_train)\n",
    "        pred=clf.predict(X_test)\n",
    "        accuracy.append(accuracy_score(Y_test, pred))\n",
    "        scores_rf.append(precision_recall_fscore_support(Y_test,pred, average='micro'))\n",
    "        \n",
    "        #print(str(count)+DataSet)\n",
    "        #print(f1_score(Y_test, pred, average='micro'))\n",
    "        count+=1\n",
    "        #print (pred)\n",
    "        #print(classification_report(Y_test,pred,labels=[0,1]))\n",
    "    accuracy = np.mean(accuracy,axis=0)   \n",
    "    return accuracy,average(scores_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes 10-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stats = pd.DataFrame(columns = ['Dataset','Accuracy', 'Precision' , 'Recall','FScore'])\n",
    "\n",
    "label='Hypertensive disease '\n",
    "asklabel='Hypertensive disease'\n",
    "result,result_labels,weighted,weighted_labels,blood,blood_labels,ask,ask_labels=prepareDataset(label,asklabel)\n",
    "res_acc,res_scores=NB10Fold(result,result_labels,\"Result\")\n",
    "wei_acc,wei_scores=NB10Fold(weighted,weighted_labels,\"Weighted\")\n",
    "blood_acc,blood_scores=NB10Fold(blood,blood_labels,\"Blood\")\n",
    "ask_acc,ask_scores=NB10Fold(ask,ask_labels,\"Ask\")\n",
    "print(\"Res: Accuracy: \" + str(res_acc)+\" # \"+ \"Scores: \"+str(res_scores))\n",
    "print(\"Wei: Accuracy: \" + str(wei_acc)+\" # \"+ \"Scores: \"+str(wei_scores))\n",
    "print(\"Bld: Accuracy: \" + str(blood_acc)+\" # \"+ \"Scores: \"+str(blood_scores))\n",
    "print(\"Ask: Accuracy: \" + str(ask_acc)+\" # \"+ \"Scores: \"+str(ask_scores))\n",
    "Stats=Stats.append({'Dataset':\"Complete\",'Accuracy':res_acc,'Precision':res_scores[0],'Recall':res_scores[1],'FScore':res_scores[2]},ignore_index=True)\n",
    "Stats=Stats.append({'Dataset':\"Weighted\",'Accuracy':wei_acc,'Precision':wei_scores[0],'Recall':wei_scores[1],'FScore':wei_scores[2]},ignore_index=True)\n",
    "Stats=Stats.append({'Dataset':\"Blood\",'Accuracy':blood_acc,'Precision':blood_scores[0],'Recall':blood_scores[1],'FScore':blood_scores[2]},ignore_index=True)\n",
    "Stats=Stats.append({'Dataset':\"AskAPatient\",'Accuracy':ask_acc,'Precision':ask_scores[0],'Recall':ask_scores[1],'FScore':ask_scores[2]},ignore_index=True)\n",
    "Stats.to_excel('NBresult_Stats10Fold'+label+'.xlsx') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from collections import Counter\\nfrom imblearn.over_sampling import SMOTENC\\nsm = SMOTENC(random_state=42, categorical_features=[0, 1,3])\\nX_res, y_res = sm.fit_resample(result, result_labels)\\nprint(Counter(result_labels))\\nprint(Counter(y_res))\\nprint(X_res)\\nprint(y_res)\\nprint(result_labels)'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from collections import Counter\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "sm = SMOTENC(random_state=42, categorical_features=[0, 1,3])\n",
    "X_res, y_res = sm.fit_resample(result, result_labels)\n",
    "print(Counter(result_labels))\n",
    "print(Counter(y_res))\n",
    "print(X_res)\n",
    "print(y_res)\n",
    "print(result_labels)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-30204082ef7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwei_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-b1dfedaeb27b>\u001b[0m in \u001b[0;36maverage\u001b[1;34m(scores)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mPrecision\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mRecall\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mFScore\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "average(wei_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_labels.value_counts())\n",
    "print(weighted_labels.value_counts())\n",
    "print(blood_labels.value_counts())\n",
    "print(ask_labels.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_labels.value_counts())\n",
    "print(weighted_labels.value_counts())\n",
    "print(blood_labels.value_counts())\n",
    "print(ask_labels.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wei_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wei_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 287,
   "position": {
    "height": "40px",
    "left": "1244px",
    "right": "20px",
    "top": "72px",
    "width": "649px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
